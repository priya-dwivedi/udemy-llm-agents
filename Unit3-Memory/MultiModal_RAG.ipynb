{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from embedchain import App\n",
    "from embedchain.models.data_type import DataType\n",
    "\n",
    "### Load the envtt file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbedChain Library\n",
    "\n",
    "Documentation: https://docs.embedchain.ai/get-started/quickstart\n",
    "\n",
    "EmbedChain is an open-source framework that makes it easy to build and deploy retrieval-augmented generation (RAG) applications powered by large language models (LLMs). Its “Conventional but Configurable” approach caters to both software and machine learning engineers.\n",
    "\n",
    "Key advantages of EmbedChain include:\n",
    "* Simplifies RAG Development: Building robust RAG pipelines involves complexities like data integration, chunking, indexing, vector storage, and more. EmbedChain streamlines this process.\n",
    "* Flexible Architecture: Choose components like LLMs, vector databases, data loaders, chunkers, and retrieval strategies to tailor the pipeline to your needs.\n",
    "* Efficient Data Handling: EmbedChain automatically loads data, generates embeddings for relevant chunks, and stores them in your chosen vector database.\n",
    "* User-Friendly APIs: Beginners can build LLM apps in just 4 lines of code, while advanced users can deeply customize the RAG pipeline.\n",
    "\n",
    "The core workflow is straightforward:\n",
    "* Add Data: Automatically load, chunk, embed, and index your data sources.\n",
    "* Query: Turn user questions into embeddings to retrieve relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "Set up your config below.\n",
    "You can define your vectordb, embedding, and llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") ## Put your OpenAI API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'vectordb': {\n",
    "    'provider': 'chroma',\n",
    "    'config': {\n",
    "    'collection_name': 'rag-collection',\n",
    "    'dir': 'db',\n",
    "    'allow_reset': True \n",
    "    }\n",
    "  },\n",
    "  'embedder': {\n",
    "    'provider': 'openai',\n",
    "    'config': {\n",
    "      'model': 'text-embedding-3-small'\n",
    "    }\n",
    "  },\n",
    "  'llm': {\n",
    "        'provider': 'openai',\n",
    "        'config': {\n",
    "            'model': 'gpt-3.5-turbo-0125',\n",
    "            'temperature': 0.5,\n",
    "            'top_p': 1,\n",
    "            'stream': False,\n",
    "            'prompt': (\n",
    "                \"Use the following pieces of context to answer the query at the end.\\n\"\n",
    "                \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\"\n",
    "                \"$context\\n\\nQuery: $query\\n\\nHelpful Answer:\"\n",
    "            ),\n",
    "            'system_prompt': (\n",
    "                \"You are an expert at looking at the provided context and answering user's query.\"\n",
    "            ),\n",
    "        }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed your documents\n",
    "\n",
    "* Supported Data Sources : https://docs.embedchain.ai/components/data-sources/overview\n",
    "* Supported LLM Models: https://docs.embedchain.ai/components/llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = App.from_config(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sources about the recently released Llama3 model\n",
    "youtube_sources = ['https://www.youtube.com/watch?v=cEHFzvU-pzk', 'https://www.youtube.com/watch?v=8Ul_0jddTU4']\n",
    "web_sources = ['https://www.theverge.com/2024/4/18/24134103/llama-3-benchmark-testing-ai-gemma-gemini-mistral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "## Add your sources to the app\n",
    "for video in youtube_sources:\n",
    "    app.add(video, data_type=DataType.YOUTUBE_VIDEO)\n",
    "\n",
    "for pdf in web_sources:\n",
    "    app.add(pdf, data_type=DataType.WEB_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Llama 3 model is available in different sizes, including an 8 billion parameter model, a 70 billion parameter model, and there is a larger model in training with 405 billion parameters.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.query(\"What different sizes is the Llama3 model avaialble in?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, Meta claims that in certain benchmarking tests, the Llama 3 8B model outperformed similarly sized models like Mistral 7B. Specifically, in the MMLU benchmark, Llama 3 8B performed significantly better than Gemma 7B and Mistral 7B. So, based on this information, the Llama3-8B model is reported to perform better than the Mistral 7B model in benchmarking tests.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.query(\"How does Llama3-8B compare to Mistral 7B model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating an open source model\n",
    "\n",
    "Use Together AI to access open source models\n",
    "\n",
    "Available inference models: https://docs.together.ai/docs/inference-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOGETHER_API_KEY\"] = os.getenv(\"TOGETHER_API_KEY\") ## Put your Together API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change the LLM in the config\n",
    "\n",
    "config = {\n",
    "  'vectordb': {\n",
    "    'provider': 'chroma',\n",
    "    'config': {\n",
    "    'collection_name': 'rag-collection-opensource',\n",
    "    'dir': 'db',\n",
    "    'allow_reset': True \n",
    "    }\n",
    "  },\n",
    "  'embedder': {\n",
    "    'provider': 'openai',\n",
    "    'config': {\n",
    "      'model': 'text-embedding-3-small'\n",
    "    }\n",
    "  },\n",
    "  'llm': {\n",
    "        'provider': 'together',\n",
    "        'config': {\n",
    "            'model': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "            'temperature': 0.5,\n",
    "            'top_p': 1,\n",
    "            'prompt': (\n",
    "                \"Use the following pieces of context to answer the query at the end.\\n\"\n",
    "                \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\"\n",
    "                \"$context\\n\\nQuery: $query\\n\\nHelpful Answer:\"\n",
    "            )\n",
    "        }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_opensource = App.from_config(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "## Add your sources to the app\n",
    "for video in youtube_sources:\n",
    "    app_opensource.add(video, data_type=DataType.YOUTUBE_VIDEO)\n",
    "\n",
    "for pdf in web_sources:\n",
    "    app_opensource.add(pdf, data_type=DataType.WEB_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdwivedi/miniconda3/envs/test/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `Together` was deprecated in LangChain 0.0.12 and will be removed in 0.3. An updated version of the class exists in the langchain-together package and should be used instead. To use it run `pip install -U langchain-together` and import as `from langchain_together import Together`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" According to Meta's blog post, Llama3-8B performs significantly better than Mistral 7B in the MMLU benchmark, which typically measures general knowledge. However, it is important to note that benchmark testing has its limitations and should be considered alongside other evaluation methods. Human evaluators also marked Llama3-8B higher than Mistral 7B in certain use cases, according to Meta.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_opensource.query(\"How does Llama3-8B compare to Mistral 7B model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Llama3, which was released by Meta, features improvements over its predecessor, Llama2. The new model has a more diverse range of responses, fewer false refusals, better reasoning abilities, and enhanced instruction understanding. Additionally, Llama3 offers both text-based and potentially multimodal responses in the future. Two sizes of Llama3 have been released: an 8B and a 70B model. These models outperform similarly sized models like Google's Gemma and Gemini, Mistral 7B, and Anthropic's Claude 3 in benchmarking tests. Meta is currently training larger versions of Llama3, which will have over 400B parameters and be capable of more complex patterns than the smaller versions. However, Meta did not release a preview of these larger models or compare them to other big models like GPT-4.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_opensource.query(\"How does Llama3 architecture differ from Llama2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Create your own RAG collection on a different topic. It can be anything like your favorite movie or a book\n",
    "- Integrate data from a few different sources like PDFs, Webpages, Videos. If there is code involved you can integrate Github too\n",
    "- Set an open source model as an LLM\n",
    "\n",
    "Test how your system does. Change configs for embeddings/retriever/different LLM and observe the difference \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
