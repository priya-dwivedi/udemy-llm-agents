{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from embedchain import App\n",
    "from embedchain.models.data_type import DataType\n",
    "\n",
    "### Load the envtt file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbedChain Library\n",
    "\n",
    "Documentation: https://docs.embedchain.ai/get-started/quickstart\n",
    "\n",
    "EmbedChain is an open-source framework that makes it easy to build and deploy retrieval-augmented generation (RAG) applications powered by large language models (LLMs). Its “Conventional but Configurable” approach caters to both software and machine learning engineers.\n",
    "\n",
    "Key advantages of EmbedChain include:\n",
    "* Simplifies RAG Development: Building robust RAG pipelines involves complexities like data integration, chunking, indexing, vector storage, and more. EmbedChain streamlines this process.\n",
    "* Flexible Architecture: Choose components like LLMs, vector databases, data loaders, chunkers, and retrieval strategies to tailor the pipeline to your needs.\n",
    "* Efficient Data Handling: EmbedChain automatically loads data, generates embeddings for relevant chunks, and stores them in your chosen vector database.\n",
    "* User-Friendly APIs: Beginners can build LLM apps in just 4 lines of code, while advanced users can deeply customize the RAG pipeline.\n",
    "\n",
    "The core workflow is straightforward:\n",
    "* Add Data: Automatically load, chunk, embed, and index your data sources.\n",
    "* Query: Turn user questions into embeddings to retrieve relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "Set up your config below.\n",
    "You can define your vectordb, embedding, and llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") ## Put your OpenAI API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'vectordb': {\n",
    "    'provider': 'chroma',\n",
    "    'config': {\n",
    "    'collection_name': 'rag-collection',\n",
    "    'dir': 'db',\n",
    "    'allow_reset': True \n",
    "    }\n",
    "  },\n",
    "  'embedder': {\n",
    "    'provider': 'openai',\n",
    "    'config': {\n",
    "      'model': 'text-embedding-3-small'\n",
    "    }\n",
    "  },\n",
    "  'llm': {\n",
    "        'provider': 'openai',\n",
    "        'config': {\n",
    "            'model': 'gpt-3.5-turbo-0125',\n",
    "            'temperature': 0.5,\n",
    "            'top_p': 1,\n",
    "            'stream': False,\n",
    "            'prompt': (\n",
    "                \"Use the following pieces of context to answer the query at the end.\\n\"\n",
    "                \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\"\n",
    "                \"$context\\n\\nQuery: $query\\n\\nHelpful Answer:\"\n",
    "            ),\n",
    "            'system_prompt': (\n",
    "                \"You are an expert at looking at the provided context and answering user's query.\"\n",
    "            ),\n",
    "        }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed your documents\n",
    "\n",
    "* Supported Data Sources : https://docs.embedchain.ai/components/data-sources/overview\n",
    "* Supported LLM Models: https://docs.embedchain.ai/components/llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = App.from_config(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sources about the recently released Llama3 model\n",
    "youtube_sources = ['https://www.youtube.com/watch?v=cEHFzvU-pzk', 'https://www.youtube.com/watch?v=8Ul_0jddTU4']\n",
    "web_sources = ['https://www.theverge.com/2024/4/18/24134103/llama-3-benchmark-testing-ai-gemma-gemini-mistral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "## Add your sources to the app\n",
    "for video in youtube_sources:\n",
    "    app.add(video, data_type=DataType.YOUTUBE_VIDEO)\n",
    "\n",
    "for pdf in web_sources:\n",
    "    app.add(pdf, data_type=DataType.WEB_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Llama3 model is available in different sizes, including an 8 billion parameter model, a 70 billion parameter model, and a 405 billion parameter model.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.query(\"What different sizes is the Llama3 model avaialble in?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, Meta claims that Llama3-8B outperformed Mistral 7B in certain benchmarking tests. In the MMLU benchmark, Llama3-8B performed significantly better than Gemma 7B and Mistral 7B. Therefore, based on this information, Llama3-8B is considered to be superior to the Mistral 7B model in specific benchmarking tests.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.query(\"How does Llama3-8B compare to Mistral 7B model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating an open source model\n",
    "\n",
    "Use Together AI to access open source models\n",
    "\n",
    "Available inference models: https://docs.together.ai/docs/inference-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOGETHER_API_KEY\"] = os.getenv(\"TOGETHER_API_KEY\") ## Put your Together API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change the LLM in the config\n",
    "\n",
    "config = {\n",
    "  'vectordb': {\n",
    "    'provider': 'chroma',\n",
    "    'config': {\n",
    "    'collection_name': 'rag-collection-opensource',\n",
    "    'dir': 'db',\n",
    "    'allow_reset': True \n",
    "    }\n",
    "  },\n",
    "  'embedder': {\n",
    "    'provider': 'openai',\n",
    "    'config': {\n",
    "      'model': 'text-embedding-3-small'\n",
    "    }\n",
    "  },\n",
    "  'llm': {\n",
    "        'provider': 'together',\n",
    "        'config': {\n",
    "            'model': 'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "            'temperature': 0.5,\n",
    "            'top_p': 1,\n",
    "            'prompt': (\n",
    "                \"Use the following pieces of context to answer the query at the end.\\n\"\n",
    "                \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\"\n",
    "                \"$context\\n\\nQuery: $query\\n\\nHelpful Answer:\"\n",
    "            )\n",
    "        }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_opensource = App.from_config(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting batches in chromadb: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "## Add your sources to the app\n",
    "for video in youtube_sources:\n",
    "    app_opensource.add(video, data_type=DataType.YOUTUBE_VIDEO)\n",
    "\n",
    "for pdf in web_sources:\n",
    "    app_opensource.add(pdf, data_type=DataType.WEB_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdwivedi/miniconda3/envs/llm_agents/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.together.Together` was deprecated in langchain-community 0.0.12 and will be removed in 0.2. An updated version of the class exists in the langchain-together package and should be used instead. To use it run `pip install -U langchain-together` and import as `from langchain_together import Together`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" According to Meta's benchmarking tests, Llama3-8B outperforms Mistral 7B in the MMLU benchmark, which measures general knowledge. Llama3-8B showed more diversity in answering prompts, had fewer false refusals, and could reason better than Mistral 7B in this test. However, it's important to note that benchmark testing AI models is imperfect, and the datasets used to benchmark models can have limitations.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_opensource.query(\"How does Llama3-8B compare to Mistral 7B model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Llama3 models have a larger parameter size compared to Llama2 models. The smallest Llama3 model, which is 8B, is already outperforming the largest Llama2 model. Meta claims that both sizes of Llama3 show more diversity in answering prompts, have fewer false refusals, and can reason better than Llama2. Additionally, Llama3 is expected to offer multimodal responses like generating images or transcribing audio files in future larger versions. The larger versions of Llama3, which are over 400B parameters, are currently training and are expected to ideally learn more complex patterns than the smaller versions. Meta did not release a preview of these larger models or compare them to other big models like GPT-4.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_opensource.query(\"How does Llama3 architecture differ from Llama2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Create your own RAG collection on a different topic. It can be anything like your favorite movie or a book\n",
    "- Integrate data from a few different sources like PDFs, Webpages, Videos. If there is code involved you can integrate Github too\n",
    "- Set an open source model as an LLM\n",
    "\n",
    "Test how your system does. Change configs for embeddings/retriever/different LLM and observe the difference \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
