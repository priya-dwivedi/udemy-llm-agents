{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") ## Put your OpenAI API key here\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\") ## Put your Tavily Search API key here\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") ## Put your Langsmith API key here\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") ## Put your Langsmith API key here\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") ## Put your Google API key here. To try Gemini Pro\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true' ## Set this as True\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = 'https://api.smith.langchain.com/' ## Set this as: https://api.smith.langchain.com/\n",
    "os.environ[\"LANGCHAIN_HUB_API_URL\"] = 'https://api.hub.langchain.com' ## Set this as : https://api.hub.langchain.com\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'llm-agents-memory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RAG Pipeline\n",
    "\n",
    "![Naive-RAG](images/naive-rag.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BGE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "This dataset has 108 arxiv papers with content parsed using Meta's Nougat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "      <th>content</th>\n",
       "      <th>noref_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2206.02336</td>\n",
       "      <td>2206.02336</td>\n",
       "      <td>Making Large Language Models Better Reasoners ...</td>\n",
       "      <td>Few-shot learning is a challenging task that r...</td>\n",
       "      <td>http://arxiv.org/pdf/2206.02336</td>\n",
       "      <td>['Yifei Li' 'Zeqi Lin' 'Shizhuo Zhang' 'Qiang ...</td>\n",
       "      <td>['cs.CL' 'cs.AI']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20220606</td>\n",
       "      <td>20230524</td>\n",
       "      <td>\\n\\n* D. Andor, L. He, K. Lee, and E. Pitler (...</td>\n",
       "      <td># Making Large Language Models Better Reasoner...</td>\n",
       "      <td># Making Large Language Models Better Reasoner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2206.04615</td>\n",
       "      <td>2206.04615</td>\n",
       "      <td>Beyond the Imitation Game: Quantifying and ext...</td>\n",
       "      <td>Language models demonstrate both quantitative ...</td>\n",
       "      <td>http://arxiv.org/pdf/2206.04615</td>\n",
       "      <td>['Aarohi Srivastava' 'Abhinav Rastogi' 'Abhish...</td>\n",
       "      <td>['cs.CL' 'cs.AI' 'cs.CY' 'cs.LG' 'stat.ML']</td>\n",
       "      <td>27 pages, 17 figures + references and appendic...</td>\n",
       "      <td>Transactions on Machine Learning Research, May...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20220609</td>\n",
       "      <td>20230612</td>\n",
       "      <td>\\n\\n* Wikiquote et al. (2021) Wikiquote, russi...</td>\n",
       "      <td># Beyond the Imitation Game: Quantifying and e...</td>\n",
       "      <td># Beyond the Imitation Game: Quantifying and e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2206.05229</td>\n",
       "      <td>2206.05229</td>\n",
       "      <td>Measuring the Carbon Intensity of AI in Cloud ...</td>\n",
       "      <td>By providing unprecedented access to computati...</td>\n",
       "      <td>http://arxiv.org/pdf/2206.05229</td>\n",
       "      <td>['Jesse Dodge' 'Taylor Prewitt' 'Remi Tachet D...</td>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>In ACM Conference on Fairness, Accountability,...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>20220610</td>\n",
       "      <td>20220610</td>\n",
       "      <td>\\n\\n* (1)\\n* Anthony et al. (2020) Lasse F. Wo...</td>\n",
       "      <td>[MISSING_PAGE_EMPTY:1]\\n\\nIntroduction\\n\\nClim...</td>\n",
       "      <td>[MISSING_PAGE_EMPTY:1]\\n\\nIntroduction\\n\\nClim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2206.05802</td>\n",
       "      <td>2206.05802</td>\n",
       "      <td>Self-critiquing models for assisting human eva...</td>\n",
       "      <td>We fine-tune large language models to write na...</td>\n",
       "      <td>http://arxiv.org/pdf/2206.05802</td>\n",
       "      <td>['William Saunders' 'Catherine Yeh' 'Jeff Wu' ...</td>\n",
       "      <td>['cs.CL' 'cs.LG']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20220612</td>\n",
       "      <td>20220614</td>\n",
       "      <td>(RLHP) has become more common [1, 2, 3, 4], d...</td>\n",
       "      <td># Self-critiquing models for assisting human e...</td>\n",
       "      <td># Self-critiquing models for assisting human e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2206.06336</td>\n",
       "      <td>2206.06336</td>\n",
       "      <td>Language Models are General-Purpose Interfaces</td>\n",
       "      <td>Foundation models have received much attention...</td>\n",
       "      <td>http://arxiv.org/pdf/2206.06336</td>\n",
       "      <td>['Yaru Hao' 'Haoyu Song' 'Li Dong' 'Shaohan Hu...</td>\n",
       "      <td>['cs.CL']</td>\n",
       "      <td>32 pages. The first three authors contribute e...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20220613</td>\n",
       "      <td>20220613</td>\n",
       "      <td>\\n\\n* Agrawal et al. (2019) Harsh Agrawal, Kar...</td>\n",
       "      <td># Language Models are General-Purpose Interfac...</td>\n",
       "      <td># Language Models are General-Purpose Interfac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doi          id                                              title  \\\n",
       "0  2206.02336  2206.02336  Making Large Language Models Better Reasoners ...   \n",
       "1  2206.04615  2206.04615  Beyond the Imitation Game: Quantifying and ext...   \n",
       "2  2206.05229  2206.05229  Measuring the Carbon Intensity of AI in Cloud ...   \n",
       "3  2206.05802  2206.05802  Self-critiquing models for assisting human eva...   \n",
       "4  2206.06336  2206.06336     Language Models are General-Purpose Interfaces   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Few-shot learning is a challenging task that r...   \n",
       "1  Language models demonstrate both quantitative ...   \n",
       "2  By providing unprecedented access to computati...   \n",
       "3  We fine-tune large language models to write na...   \n",
       "4  Foundation models have received much attention...   \n",
       "\n",
       "                            source  \\\n",
       "0  http://arxiv.org/pdf/2206.02336   \n",
       "1  http://arxiv.org/pdf/2206.04615   \n",
       "2  http://arxiv.org/pdf/2206.05229   \n",
       "3  http://arxiv.org/pdf/2206.05802   \n",
       "4  http://arxiv.org/pdf/2206.06336   \n",
       "\n",
       "                                             authors  \\\n",
       "0  ['Yifei Li' 'Zeqi Lin' 'Shizhuo Zhang' 'Qiang ...   \n",
       "1  ['Aarohi Srivastava' 'Abhinav Rastogi' 'Abhish...   \n",
       "2  ['Jesse Dodge' 'Taylor Prewitt' 'Remi Tachet D...   \n",
       "3  ['William Saunders' 'Catherine Yeh' 'Jeff Wu' ...   \n",
       "4  ['Yaru Hao' 'Haoyu Song' 'Li Dong' 'Shaohan Hu...   \n",
       "\n",
       "                                    categories  \\\n",
       "0                            ['cs.CL' 'cs.AI']   \n",
       "1  ['cs.CL' 'cs.AI' 'cs.CY' 'cs.LG' 'stat.ML']   \n",
       "2                                    ['cs.LG']   \n",
       "3                            ['cs.CL' 'cs.LG']   \n",
       "4                                    ['cs.CL']   \n",
       "\n",
       "                                             comment  \\\n",
       "0                                               None   \n",
       "1  27 pages, 17 figures + references and appendic...   \n",
       "2  In ACM Conference on Fairness, Accountability,...   \n",
       "3                                               None   \n",
       "4  32 pages. The first three authors contribute e...   \n",
       "\n",
       "                                         journal_ref primary_category  \\\n",
       "0                                               None            cs.CL   \n",
       "1  Transactions on Machine Learning Research, May...            cs.CL   \n",
       "2                                               None            cs.LG   \n",
       "3                                               None            cs.CL   \n",
       "4                                               None            cs.CL   \n",
       "\n",
       "   published   updated                                         references  \\\n",
       "0   20220606  20230524  \\n\\n* D. Andor, L. He, K. Lee, and E. Pitler (...   \n",
       "1   20220609  20230612  \\n\\n* Wikiquote et al. (2021) Wikiquote, russi...   \n",
       "2   20220610  20220610  \\n\\n* (1)\\n* Anthony et al. (2020) Lasse F. Wo...   \n",
       "3   20220612  20220614   (RLHP) has become more common [1, 2, 3, 4], d...   \n",
       "4   20220613  20220613  \\n\\n* Agrawal et al. (2019) Harsh Agrawal, Kar...   \n",
       "\n",
       "                                             content  \\\n",
       "0  # Making Large Language Models Better Reasoner...   \n",
       "1  # Beyond the Imitation Game: Quantifying and e...   \n",
       "2  [MISSING_PAGE_EMPTY:1]\\n\\nIntroduction\\n\\nClim...   \n",
       "3  # Self-critiquing models for assisting human e...   \n",
       "4  # Language Models are General-Purpose Interfac...   \n",
       "\n",
       "                                       noref_content  \n",
       "0  # Making Large Language Models Better Reasoner...  \n",
       "1  # Beyond the Imitation Game: Quantifying and e...  \n",
       "2  [MISSING_PAGE_EMPTY:1]\\n\\nIntroduction\\n\\nClim...  \n",
       "3  # Self-critiquing models for assisting human e...  \n",
       "4  # Language Models are General-Purpose Interfac...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Hugging Face dataset\n",
    "dataset = load_dataset(\"deep-learning-analytics/arxiv_small_nougat\")\n",
    "\n",
    "# Convert to a Pandas DataFrame\n",
    "df = Dataset.to_pandas(dataset['train'])\n",
    "\n",
    "# Preview the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2206.02336, 2206.04615, 2206.05229, 2206.05802, 2206.06336, 2206.07635, 2206.14858, 2207.0056, 2207.04672, 2207.05221, 2207.05608, 2207.09983, 2207.10551, 2208.02294, 2208.03299, 2208.11663, 2208.14271, 2209.03143, 2209.07686, 2209.07753, 2209.07858, 2209.14375, 2209.15003, 2210.01241, 2210.02406, 2210.02414, 2210.02875, 2210.02969, 2210.0307, 2210.03078, 2210.0335, 2210.03493, 2210.03629, 2210.03945, 2210.05359, 2210.06245, 2210.07316, 2210.07382, 2210.077, 2210.09261, 2210.11399, 2210.11416, 2210.12283, 2210.13236, 2211.00053, 2211.00295, 2211.01786, 2211.0191, 2211.02001, 2211.04325, 2211.051, 2211.08264, 2211.08411, 2211.09085, 2211.0911, 2211.0926, 2211.10435, 2211.11736, 2212.00193, 2212.06817, 2212.08073, 2212.08286, 2212.0841, 2212.09689, 2212.10403, 2212.1056, 2212.12017, 2212.14882, 2301.00303, 2301.03728, 2301.07597, 2301.08653, 2301.09211, 2301.10226, 2301.11305, 2301.12867, 2301.13196, 2301.13688, 2302.04166, 2302.04761, 2302.07459, 2302.07736, 2302.07842, 2302.07867, 2302.08582, 2302.0927, 2302.13971, 2303.11156, 2303.12712, 2303.15056, 2303.17651, 2304.01196, 2304.01373, 2304.03277, 2304.06364, 2304.07327, 2304.09542, 2304.12298, 2304.14178, 2305.02301, 2305.03047, 2305.098, 2305.09941, 2305.11206, 2305.15717, 2305.17493, 2306.05949, 2307.09288]\n"
     ]
    }
   ],
   "source": [
    "paper_ids = df['doi'].unique().tolist()\n",
    "print(paper_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset of data to load into a database\n",
    "\n",
    "* Our key text column will be the `noref_content` which has the content of the paper without the references\n",
    "* We will include some metadata fields as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "keep_cols = ['id', 'title', 'authors', 'summary', 'source', 'published', 'noref_content']\n",
    "df_subset = df[keep_cols]\n",
    "df_subset= df_subset.dropna()\n",
    "df_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader\n",
    "loader = DataFrameLoader(df_subset, page_content_column=\"noref_content\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement your text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Split Docs:  9790\n"
     ]
    }
   ],
   "source": [
    "print(\"Num Split Docs: \", len(split_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement your embedding and Vector Store generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load to a database - Run this for the first time to create the Db file\n",
    "\n",
    "# db = Chroma.from_documents(split_docs, bge_embeddings, persist_directory=\"./chroma_db\")\n",
    "\n",
    "## Load from a database\n",
    "db = Chroma(persist_directory=\"./chroma_db\", embedding_function=bge_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched doc 1 is :  RLHF has emerged as a powerful strategy for fine-tuning Large Language Models, enabling significant improvements in their performance (Christiano et al., 2017). The method, first showcased by Stiennon et al. (2020) in the context of text-summarization tasks, has since been extended to a range of other applications. In this paradigm, models are fine-tuned based on feedback from human users, thus iteratively aligning the models' responses more closely with human expectations and preferences. /n ===========\n",
      "Matched doc 2 is :  Christiano et al., 2017; Warnell et al., 2018) aims to overcome these limitations by using human preferences as an evaluation metric and as an objective function to optimize the language model. Using RLHF allows LMs to be more closely aligned with complex human preferences and values which are difficult to capture by hard-coded reward functions. /n ===========\n",
      "Matched doc 3 is :  A successful example of RLHF used to teach a LM to use an external tool stems from _WebGPT_Nakano et al. (2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing references to support such answers. The tool interface is a simplified text-based web-browser. The model architecture is based on _GPT3_(Brown et al., 2020) and is trained to perform browsing actions expressed in natural language. The model is fine-tuned on question-human demonstration pairs, before further optimization via RLHF. On two QA datasets, _WebGPT_'s answers are preferred relative to human-generated ones and tend to be more factual than the original vanilla _GPT3_ model. Similarly, Menick et al. (2022) propose _GopherCite_, a _Gopher_-based LM model (Rae et al., 2021) fine-tuned with RLHF that can cite supporting evidence when answering questions and abstain from answering when unsure. In contrast with _WebGPT_, _GopherCite_ uses an information retrieval external module /n ===========\n",
      "Matched doc 4 is :  when unsure. In contrast with _WebGPT_, _GopherCite_ uses an information retrieval external module rather than a web-browser to find relevant information that improves its question answering capabilities. Besides learning to use external tools, RLHF has also proven useful for a wide range of language generation tasks, from summarization (Ziegler et al., 2019; Wu et al., 2021; Stiennon et al., 2020) to training more helpful, harmless, and accurate assistants (Glaese et al., 2022; Cohen et al., 2022; Ouyang et al., 2022; Bai et al., 2022). Since these works do not focus on training models to reason and act, they are out of the scope of this survey. /n ===========\n"
     ]
    }
   ],
   "source": [
    "# query it\n",
    "query = \"What is RLHF? When can it be used?\"\n",
    "matched_docs = db.similarity_search(query, k=8)\n",
    "\n",
    "# print results\n",
    "for index, value in enumerate(matched_docs):\n",
    "    pos = index+1\n",
    "    if index <=3:\n",
    "        print(f\"Matched doc {pos} is : \", matched_docs[index].page_content, \"/n ===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "I prefer answers to be 8-10 sentences long:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdwivedi/miniconda3/envs/llm_agents/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['k'] = 8\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(llm=model, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  The benefit of RLHF compared to other techniques lies in its ability to fine-tune Large Language Models (LMs) by utilizing human feedback to iteratively align the model's responses more closely with human expectations and preferences. RLHF works by generating text using a pre-trained LM, which is then evaluated by humans to learn a reward model that captures human preferences when judging model output. This approach allows LMs to be more closely aligned with complex human preferences and values that are difficult to capture with hard-coded reward functions. RLHF can be applied on top of a general-purpose LM pre-trained via self-supervised learning, and it can also be used after an initial supervised fine-tuning phase using expert demonstrations for more complex tasks. The method has shown significant improvements in LM performance across various applications, showcasing its power in enhancing model capabilities. Additionally, RLHF enables models to perform tasks that require external tools or information retrieval modules, showcasing its versatility and effectiveness in improving LM performance in real-world applications. Through the use of RLHF, models like _WebGPT_ and _GopherCite_ have been able to provide answers and citations supported by external tools and evidence, demonstrating the practical benefits of RLHF in enhancing LM functionality. Overall, RLHF stands out as a powerful strategy for fine-tuning LMs, offering a unique approach that leverages human feedback to optimize model performance and align model responses with human preferences more effectively compared to traditional techniques.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the benefit of RLHF compared to other techniques?\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer: \", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Model collapse is a degenerative process in learning where generations of generative models end up polluting the training set of the next generation with generated data, leading to a misinterpretation of reality. This phenomenon occurs when models start forgetting improbable events and converge to a distribution that deviates significantly from the original one. It is different from catastrophic forgetting, which refers to the model forgetting previously learned data when exposed to new information. In model collapse, the models do not forget previous data but misinterpret what they believe to be real, reinforcing their own beliefs. The primary cause of model collapse is the compounding of errors over generations, leading to divergence from the original model. It is essential to access genuine human-generated content to prevent model collapse.\n"
     ]
    }
   ],
   "source": [
    "## Based on the paper - https://arxiv.org/pdf/2305.17493.pdf\n",
    "\n",
    "query = \"What is model collapse? Why does it occur and how is it different from catastrophic forgetting?\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer: \", result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  The RLHF training for the Llama2-70B chat model involved iteratively refining the model using methodologies like rejection sampling and Proximal Policy Optimization (PPO). During this stage, the model accumulated iterative reward modeling data in parallel with enhancements to ensure the reward models remained within distribution. The process focused on optimizing the model for better human preference alignment, helpfulness, and safety by leveraging response scores as rewards. To address the trade-off between helpfulness and safety, two separate reward models were trained, one for each aspect. The training process aimed to make the model more robust to jailbreak attempts and improve the quality of responses generated by the model in various contexts.\n"
     ]
    }
   ],
   "source": [
    "## Based on the paper - https://arxiv.org/abs/2307.09288\n",
    "\n",
    "query = \"Explain in detail how was RLHF training done for Llama2-70B chat model?\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer: \", result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long context modeling using Gemini\n",
    "\n",
    "Get API for Gemini from https://aistudio.google.com/app/prompts/new_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "GOOGLE_API_KEY= os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest planet in our Solar System is **Jupiter**! 🪐  It's so large that all of the other planets could fit inside it.  It's known for its Great Red Spot, which is a giant storm that has been going on for hundreds of years. Jupiter is made mostly of gas, and it has beautiful swirling clouds of different colors.  It also has a lot of moons, with the four largest being Io, Europa, Ganymede, and Callisto. 🔭 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gemini_llm = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\")\n",
    "response = gemini_llm.generate_content(\"Tell me about the biggest planet in our Solar System?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def count_token(text):\n",
    "    # Initialize the tokenizer\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    # Tokenize the text\n",
    "    tokens = encoding.encode(text,allowed_special={'<|endoftext|>', '<|endofprompt|>'})\n",
    "    # Count the number of tokens\n",
    "    number_of_tokens = len(tokens)\n",
    "    # Print the number of tokens\n",
    "    print(\"Number of tokens:\", number_of_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 388890\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "total_content = df.tail(25)['noref_content'].str.cat(sep='/n')\n",
    "print(count_token(total_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(question, context=total_content):\n",
    "    prompt = f\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        I prefer answers to be 8-10 sentences long:\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 388965\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "query = \"What is model collapse? Why does it occur and how is it different from catastrophic forgetting?\"\n",
    "prompt = generate_prompt(query)\n",
    "print(count_token(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model collapse is a phenomenon that can occur when large language models (LLMs) are trained on data that has been generated by other LLMs. This can cause the models to progressively forget the original data distribution, leading to a decline in performance over generations. The primary cause of model collapse is statistical error, which arises due to the finite number of samples used in training. As models are trained on data generated by previous generations, these errors can compound, causing the models to drift further and further away from the true data distribution. Unlike catastrophic forgetting, where models forget previously learned information when learning new information, model collapse involves models misinterpreting what they believe to be real by reinforcing their own beliefs based on the data generated by previous models. To prevent model collapse, it is crucial to have access to genuine human-generated content and to ensure that the training data is not overly reliant on model-generated data. \n",
      "\n",
      "CPU times: user 21.6 ms, sys: 35.9 ms, total: 57.5 ms\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = gemini_llm.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 388986\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain in detail how RLHF training was done for Llama2-70B chat model. If you don't know the answer, just say so. Don't try to make it up\"\n",
    "prompt = generate_prompt(query)\n",
    "count_token(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2-Chat was fine-tuned using Reinforcement Learning with Human Feedback (RLHF).  This process begins with collecting human preference data.  Annotators provide prompts and compare two model generations, selecting the response that is most helpful while also being safe.  Using this data, a reward model is trained with a pair-wise ranking loss that learns to assign higher scores to the preferred responses.  The model is then fine-tuned with RLHF, using either Proximal Policy Optimization (PPO) or Rejection Sampling.  PPO directly optimizes the model's policy using the reward model as a reward function.  In Rejection Sampling, multiple outputs are sampled from the model for a prompt, and the response with the highest reward score is then used as the new gold standard. The model is then fine-tuned using supervised learning with this new set of ranked responses. To improve consistency in multi-turn dialogues, we use Ghost Attention (GAtt).  GAtt prefixes a system prompt, such as \"You are a safe and honest assistant,\" to each dialogue.  The model is then fine-tuned on the resulting safer outputs without the preprompt.  This distills the context of the system prompt into the model and helps it to remember instructions over the course of a dialogue.  To increase long-tail safety, we also fine-tune the model on adversarial prompts designed to elicit unsafe behavior.\n",
      "CPU times: user 20.7 ms, sys: 34.3 ms, total: 55 ms\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = gemini_llm.generate_content(prompt)\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
