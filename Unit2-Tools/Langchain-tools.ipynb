{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Tools Library\n",
    "Langchain has built integration with a lot of different tools. You can see their current list of integrations below.\n",
    "\n",
    "https://python.langchain.com/docs/integrations/tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arxiv',\n",
       " 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun, ArxivAPIWrapper\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=ArxivAPIWrapper())\n",
    "arxiv_tool.name, arxiv_tool.description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2024-04-11\\nTitle: Me LLaMA: Foundation Large Language Models for Medical Applications\\nAuthors: Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Xinyu Zhou, Huan He, Lucila Ohno-Machado, Yonghui Wu, Hua Xu, Jiang Bian\\nSummary: Recent advancements in large language models (LLMs) such as ChatGPT and LLaMA\\nhave hinted at their potential to revolutionize medical applications, yet their\\napplication in clinical settings often reveals limitations due to a lack of\\nspecialized training on medical-specific data. In response to this challenge,\\nthis study introduces Me-LLaMA, a novel medical LLM family that includes\\nfoundation models - Me-LLaMA 13/70B, along with their chat-enhanced versions -\\nMe-LLaMA 13/70B-chat, developed through continual pre-training and instruction\\ntuning of LLaMA2 using large medical datasets. Our methodology leverages a\\ncomprehensive domain-specific data suite, including a large-scale, continual\\npre-training dataset with 129B tokens, an instruction tuning dataset with 214k\\nsamples, and a new medical evaluation benchmark (MIBE) across six critical\\nmedical tasks with 12 datasets. Our extensive evaluation using the MIBE shows\\nthat Me-LLaMA models achieve overall better performance than existing\\nopen-source medical LLMs in zero-shot, few-shot and supervised learning\\nabilities. With task-specific instruction tuning, Me-LLaMA models outperform\\nChatGPT on 7 out of 8 datasets and GPT-4 on 5 out of 8 datasets. In addition,\\nwe investigated the catastrophic forgetting problem, and our results show that\\nMe-LLaMA models outperform other open-source medical LLMs in mitigating this\\nissue. Me-LLaMA is one of the largest open-source medical foundation LLMs that\\nuse both biomedical and clinical data. It exhibits superior performance across\\nboth general and medical tasks compared to other open-source medical LLMs,\\nrendering it an attractive choice for medical AI applications. We release our\\nmodels, datasets, and evaluation scripts at:\\nhttps://github.com/BIDS-Xu-Lab/Me-LLaMA.\\n\\nPublished: 2024-05-13\\nTitle: Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs\\nAuthors: Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott\\nSummary: Text generation with Large Language Models (LLMs) is known to be memory bound\\ndue to the combination of their auto-regressive nature, huge parameter counts,\\nand limited memory bandwidths, often resulting in low token rates. Speculative\\ndecoding has been proposed as a solution for LLM inference acceleration.\\nHowever, since draft models are often unavailable in the modern open-source LLM\\nfamilies, e.g., for Llama 2 7B, training a high-quality draft model is required\\nto enable inference acceleration via speculative decoding. In this paper, we\\npropose a simple draft model training framework for direct alignment to\\nchat-capable target models. With the proposed framework, we train Llama 2 Chat\\nDrafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\\\\% of\\nthe original size. Our training framework only consists of pretraining,\\ndistillation dataset generation, and finetuning with knowledge distillation,\\nwith no additional alignment procedure. For the finetuning step, we use\\ninstruction-response pairs generated by target model for distillation in\\nplausible data distribution, and propose a new Total Variation Distance++\\n(TVD++) loss that incorporates variance reduction techniques inspired from the\\npolicy gradient method in reinforcement learning. Our empirical results show\\nthat Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3\\nblock efficiency and 2.4$\\\\times$ speed-up relative to autoregressive decoding\\non various tasks with no further task-specific fine-tuning.\\n\\nPublished: 2024-02-01\\nTitle: Investigating Bias Representations in Llama 2 Chat via Activation Steering\\nAuthors: Dawn Lu, Nina Rimsky\\nSummary: We address the challenge of soci'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_tool.run(\"Tell me about Llama 2: Open Foundation and Fine-Tuned Chat Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikidata Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Wikidata',\n",
       " 'A wrapper around Wikidata. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be the exact name of the item you want information about or a Wikidata QID.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.wikidata.tool import WikidataAPIWrapper, WikidataQueryRun\n",
    "\n",
    "wikidata_tool = WikidataQueryRun(api_wrapper=WikidataAPIWrapper())\n",
    "wikidata_tool.name, wikidata_tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Q7251:\n",
      "Label: Alan Turing\n",
      "Description: English computer scientist (1912–1954)\n",
      "Aliases: Alan M. Turing, Alan Mathieson Turing, Turing, Alan Mathison Turing\n",
      "instance of: human\n",
      "country of citizenship: United Kingdom\n",
      "occupation: computer scientist, mathematician, university teacher, cryptographer, logician, statistician, marathon runner, artificial intelligence researcher\n",
      "sex or gender: male\n",
      "date of birth: 1912-06-23\n",
      "date of death: 1954-06-07\n",
      "sport: athletics\n",
      "place of birth: Maida Vale, Warrington Lodge\n",
      "educated at: King's College, Princeton University, Sherborne School, Hazlehurst Community Primary School\n",
      "employer: Victoria University of Manchester, Government Communications Headquarters, University of Cambridge, National Physical Laboratory (United Kingdom)\n",
      "place of death: Wilmslow\n",
      "field of work: cryptanalysis, computer science, mathematics, logic, cryptography\n",
      "cause of death: cyanide poisoning\n",
      "notable work: On Computable Numbers, with an Application to the Entscheidungsproblem, Computing Machinery and Intelligence, Intelligent Machinery, halting problem, Turing machine, Turing test, Turing completeness, Church-Turing thesis, universal Turing machine, Symmetric Turing machine, non-deterministic Turing machine, Bombe, probabilistic Turing machine, Turing degree\n",
      "religion or worldview: atheism\n",
      "mother: Ethel Sara Stoney\n",
      "father: Julius Mathison Turing\n",
      "doctoral student: Robin Gandy, Beatrice Helen Worsley\n",
      "student: Robin Gandy\n",
      "\n",
      "Result Q28846012:\n",
      "Label: Alan Turing\n",
      "Description: fictional analogon of Alan Turing (1912-1954)\n",
      "Aliases: Alan Mathison Turing\n",
      "instance of: fictional human\n",
      "sex or gender: male\n"
     ]
    }
   ],
   "source": [
    "print(wikidata_tool.run(\"Alan Turing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('youtube_search',\n",
       " 'search for youtube videos associated with a person. the input to this tool should be a comma separated list, the first part contains a person name and the second a number that is the maximum number of video results to return aka num_results. the second part is optional')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "youtube_tool = YouTubeSearchTool()\n",
    "youtube_tool.name, youtube_tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['https://www.youtube.com/watch?v=h6Udx4UOGvA&pp=ygUPc3RlcGhlbiBjb2xiZXJ0', 'https://www.youtube.com/watch?v=HQIqWLu0yA8&pp=ygUPc3RlcGhlbiBjb2xiZXJ0', 'https://www.youtube.com/watch?v=aM3CVhgnw68&pp=ygUPc3RlcGhlbiBjb2xiZXJ0', 'https://www.youtube.com/watch?v=HeJH28Zs1iM&pp=ygUPc3RlcGhlbiBjb2xiZXJ0', 'https://www.youtube.com/watch?v=s6uxSyJYlVI&pp=ygUPc3RlcGhlbiBjb2xiZXJ0']\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_tool.run(\"stephen colbert,5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your custom tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File writing tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "class FileTools():\n",
    "\n",
    "  @tool(\"Write File with content\")\n",
    "  def write_file(data):\n",
    "    \"\"\"Useful to write a file to a given path with a given content. \n",
    "       The input to this tool should be a pipe (|) separated text \n",
    "       of length two. The first string is the location of the file and the second is the message in it.\n",
    "       For example, `./observations.txt|REACT_COMPONENT_CODE_PLACEHOLDER`.\n",
    "       Replace REACT_COMPONENT_CODE_PLACEHOLDER with the actual \n",
    "       code you want to write to the file.\"\"\"\n",
    "    try:\n",
    "      path, content = data.split(\"|\")\n",
    "      path = path.replace(\"\\n\", \"\").replace(\" \", \"\").replace(\"`\", \"\")\n",
    "      with open(path, \"w\") as f:\n",
    "        f.write(content)\n",
    "      return f\"File written to {path}.\"\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      return \"Error with the input format for the tool.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Write File with content',\n",
       " 'Write File with content(data) - Useful to write a file to a given path with a given content. \\n       The input to this tool should be a pipe (|) separated text \\n       of length two. The first string is the location of the file and the second is the message in it.\\n       For example, `./observations.txt|REACT_COMPONENT_CODE_PLACEHOLDER`.\\n       Replace REACT_COMPONENT_CODE_PLACEHOLDER with the actual \\n       code you want to write to the file.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_tool = FileTools.write_file\n",
    "file_tool.name, file_tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File written to ./test.txt.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_tool.run('./test.txt|This is a test for the tool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Calculator tool with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from math import radians, sin\n",
    "\n",
    "desc = (\n",
    "    '''use this tool when you need to calculate the area of a parallelogram.\n",
    "     If the parallelogram is a rectangle then you will be given just width and height.\n",
    "     If it not a rectangle, then you will need the angle between the width and the height. Angle should be in degrees.\n",
    "     To use the tool, you must provide at least two of these 3 parameters- ['width', 'height', 'angle'].\n",
    "     The input to the tool must be provided as width|height|angle or width|height\n",
    "    '''\n",
    ")\n",
    "\n",
    "class AreaCalculator(BaseTool):\n",
    "    name = \"Parallelogram Area Calculator\"\n",
    "    description = desc\n",
    "    \n",
    "    def _run(\n",
    "        self,\n",
    "        input\n",
    "    ):\n",
    "        # check for the values we have been given\n",
    "        extract = input.split(\"|\")\n",
    "        width = float(extract[0])\n",
    "        height = float(extract[1])\n",
    "        if len(extract) == 2:\n",
    "            angle = None\n",
    "        else:\n",
    "            angle= float(extract[2])\n",
    "        if not angle:\n",
    "            return width * height\n",
    "        elif angle:\n",
    "            return width * height * float(sin(radians(angle)))\n",
    "        else:\n",
    "            return \"Could not calculate the area of a parallelogram. Need two or more of `width`, `height`, or `angle`.\"\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "area_tool = AreaCalculator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Parallelogram Area Calculator',\n",
       " \"use this tool when you need to calculate the area of a parallelogram.\\n     If the parallelogram is a rectangle then you will be given just width and height.\\n     If it not a rectangle, then you will need the angle between the width and the height. Angle should be in degrees.\\n     To use the tool, you must provide at least two of these 3 parameters- ['width', 'height', 'angle'].\\n     The input to the tool must be provided as width|height|angle or width|height\\n    \")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_tool.name, area_tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.99999999999999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_tool.run('10|12|30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_tool.run('10|12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Make your own custom tool. Ideas include:\n",
    "- A tool to get the latest weather at a location\n",
    "- HuggingFace tool to run text classification\n",
    "\n",
    "Make sure to set a good description for the tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing multiple tool selection with Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up environment and keys for langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") ## Put your OpenAI API key here\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\") ## Put your Tavily Search API key here\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") ## Put your Langsmith API key here\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") ## Put your Langsmith API key here\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true' ## Set this as True\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = 'https://api.smith.langchain.com/' ## Set this as: https://api.smith.langchain.com/\n",
    "os.environ[\"LANGCHAIN_HUB_API_URL\"] = 'https://api.hub.langchain.com' ## Set this as : https://api.hub.langchain.com\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'llm-agents'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReACT Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReACT Paper - https://arxiv.org/pdf/2210.03629.pdf\n",
    "\n",
    "![ReACT Loop](images/ReAct_loop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0125',\n",
    "                 temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [arxiv_tool, wikidata_tool, youtube_tool, file_tool, area_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, max_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the Wikidata tool to find information about Beethoven.\n",
      "Action: Wikidata\n",
      "Action Input: Beethoven\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Error response when hitting Wikibase: Error(code='invalid-item-id', message='Not a valid item ID: <wikibase_rest_api_client.types.Unset object at 0x105ee9ac0>', additional_properties={})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3mResult Q255:\n",
      "Label: Ludwig van Beethoven\n",
      "Description: German composer (1770–1827)\n",
      "Aliases: Beethoven, L. van Beethoven, Ludvig van Beethoven, Ludwig Van Beethoven, L. Bethovenas, Ludwik van Beethoven, Louis van Beethoven, L. v. Beethoven, L. van Betkhoven, Ludwig von Beethoven\n",
      "instance of: human\n",
      "country of citizenship: Electorate of Cologne, Austrian Empire\n",
      "occupation: composer, pianist, conductor, music educator, organist, virtuoso, improviser, violinist, writer\n",
      "sex or gender: male\n",
      "date of birth: 1770-12-16, 1770-12-17, 1770\n",
      "date of death: 1827-03-26, 1827-03-26, 1827\n",
      "place of birth: Bonn\n",
      "place of death: Vienna\n",
      "field of work: classical music, symphony, string quartet, chamber music, traditional folk song, string trio, opera, music, piano performance\n",
      "cause of death: liver cirrhosis\n",
      "notable work: Für Elise, Symphony No. 9, Piano Sonata No. 14, Missa Solemnis, Piano Sonata No. 8, Symphony No. 5, Symphony No. 6, Piano Sonata No. 21, Piano Sonata No. 23, Violin Sonata No. 9, Symphony No. 3, Fidelio\n",
      "religion or worldview: Catholicism\n",
      "mother: Maria Magdalena van Beethoven\n",
      "father: Johann van Beethoven\n",
      "student: Carl Czerny, Dorothea von Ertmann, Ferdinand Ries, Archduke Rudolf of Austria, Babette Countess Keglevich de Buzin\n",
      "\n",
      "Result Q144207:\n",
      "Label: 1815 Beethoven\n",
      "Description: asteroid\n",
      "Aliases: Beethoven\n",
      "instance of: asteroid\u001b[0m\u001b[32;1m\u001b[1;3mI have found the information about Beethoven, now I need to find out when he was born.\n",
      "Action: Wikidata\n",
      "Action Input: Q255\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Error response when hitting Wikibase: Error(code='invalid-item-id', message='Not a valid item ID: <wikibase_rest_api_client.types.Unset object at 0x105ee9ac0>', additional_properties={})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3mResult Q255:\n",
      "Label: Ludwig van Beethoven\n",
      "Description: German composer (1770–1827)\n",
      "Aliases: Beethoven, L. van Beethoven, Ludvig van Beethoven, Ludwig Van Beethoven, L. Bethovenas, Ludwik van Beethoven, Louis van Beethoven, L. v. Beethoven, L. van Betkhoven, Ludwig von Beethoven\n",
      "instance of: human\n",
      "country of citizenship: Electorate of Cologne, Austrian Empire\n",
      "occupation: composer, pianist, conductor, music educator, organist, virtuoso, improviser, violinist, writer\n",
      "sex or gender: male\n",
      "date of birth: 1770-12-16, 1770-12-17, 1770\n",
      "date of death: 1827-03-26, 1827-03-26, 1827\n",
      "place of birth: Bonn\n",
      "place of death: Vienna\n",
      "field of work: classical music, symphony, string quartet, chamber music, traditional folk song, string trio, opera, music, piano performance\n",
      "cause of death: liver cirrhosis\n",
      "notable work: Für Elise, Symphony No. 9, Piano Sonata No. 14, Missa Solemnis, Piano Sonata No. 8, Symphony No. 5, Symphony No. 6, Piano Sonata No. 21, Piano Sonata No. 23, Violin Sonata No. 9, Symphony No. 3, Fidelio\n",
      "religion or worldview: Catholicism\n",
      "mother: Maria Magdalena van Beethoven\n",
      "father: Johann van Beethoven\n",
      "student: Carl Czerny, Dorothea von Ertmann, Ferdinand Ries, Archduke Rudolf of Austria, Babette Countess Keglevich de Buzin\n",
      "\n",
      "Result Q124034935:\n",
      "Label: punishment of woman who prefers mortal lover to gods\n",
      "Description: narrative motif documented in Thompson's Motif-Index of Folk-Literature\n",
      "Aliases: Q255.\n",
      "instance of: narrative motif, folklore motif\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'who is Beethoven? When was he born',\n",
       " 'output': 'Agent stopped due to iteration limit or time limit.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"who is Beethoven? When was he born\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the arxiv tool to find and summarize the paper for the user.\n",
      "Action: arxiv\n",
      "Action Input: ReACT - SYNERGIZING REASONING AND ACTING\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2023-12-15\n",
      "Title: ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\n",
      "Authors: Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, Sanjiv Kumar\n",
      "Summary: Answering complex natural language questions often necessitates multi-step\n",
      "reasoning and integrating external information. Several systems have combined\n",
      "knowledge retrieval with a large language model (LLM) to answer such questions.\n",
      "These systems, however, suffer from various failure cases, and we cannot\n",
      "directly train them end-to-end to fix such failures, as interaction with\n",
      "external knowledge is non-differentiable. To address these deficiencies, we\n",
      "define a ReAct-style LLM agent with the ability to reason and act upon external\n",
      "knowledge. We further refine the agent through a ReST-like method that\n",
      "iteratively trains on previous trajectories, employing growing-batch\n",
      "reinforcement learning with AI feedback for continuous self-improvement and\n",
      "self-distillation. Starting from a prompted large model and after just two\n",
      "iterations of the algorithm, we can produce a fine-tuned small model that\n",
      "achieves comparable performance on challenging compositional question-answering\n",
      "benchmarks with two orders of magnitude fewer parameters.\n",
      "\n",
      "Published: 2019-06-27\n",
      "Title: To Act or React: Investigating Proactive Strategies For Online Community Moderation\n",
      "Authors: Hussam Habib, Maaz Bin Musa, Fareed Zaffar, Rishab Nithyanand\n",
      "Summary: Reddit administrators have generally struggled to prevent or contain such\n",
      "discourse for several reasons including: (1) the inability for a handful of\n",
      "human administrators to track and react to millions of posts and comments per\n",
      "day and (2) fear of backlash as a consequence of administrative decisions to\n",
      "ban or quarantine hateful communities. Consequently, as shown in our background\n",
      "research, administrative actions (community bans and quarantines) are often\n",
      "taken in reaction to media pressure following offensive discourse within a\n",
      "community spilling into the real world with serious consequences. In this\n",
      "paper, we investigate the feasibility of proactive moderation on Reddit --\n",
      "i.e., proactively identifying communities at risk of committing offenses that\n",
      "previously resulted in bans for other communities. Proactive moderation\n",
      "strategies show promise for two reasons: (1) they have potential to narrow down\n",
      "the communities that administrators need to monitor for hateful content and (2)\n",
      "they give administrators a scientific rationale to back their administrative\n",
      "decisions and interventions. Our work shows that communities are constantly\n",
      "evolving in their user base and topics of discourse and that evolution into\n",
      "hateful or dangerous (i.e., considered bannable by Reddit administrators)\n",
      "communities can often be predicted months ahead of time. This makes proactive\n",
      "moderation feasible. Further, we leverage explainable machine learning to help\n",
      "identify the strongest predictors of evolution into dangerous communities. This\n",
      "provides administrators with insights into the characteristics of communities\n",
      "at risk becoming dangerous or hateful. Finally, we investigate, at scale, the\n",
      "impact of participation in hateful and dangerous subreddits and the\n",
      "effectiveness of community bans and quarantines on the behavior of members of\n",
      "these communities.\n",
      "\n",
      "Published: 2024-05-16\n",
      "Title: An Electrical Grid with Discrete Energy Levels\n",
      "Authors: H. Grebel\n",
      "Summary: Minimizing both power fluctuations and energy waste in an electrical grid is\n",
      "a central challenge to energy policy. Any discrepancy between power production\n",
      "and loads may lead to inefficiencies and instability in the system. Right now,\n",
      "the electrical grid is an analog system that only retroactively reacts to power\n",
      "demands. The balancing act becomes even harder with the penetration of\n",
      "sustainable resources (e.g., wind turbines). Here, we consider the effect of\n",
      "random perturbations to the grid's\u001b[0m\u001b[32;1m\u001b[1;3mI have found the summary of the ReACT - SYNERGIZING REASONING AND ACTING paper.\n",
      "Final Answer: The ReACT - SYNERGIZING REASONING AND ACTING paper discusses a ReAct-style LLM agent with the ability to reason and act upon external knowledge, refining the agent through a ReST-like method for continuous self-improvement and self-distillation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Summarize the ReACT - SYNERGIZING REASONING AND ACTING paper for me',\n",
       " 'output': 'The ReACT - SYNERGIZING REASONING AND ACTING paper discusses a ReAct-style LLM agent with the ability to reason and act upon external knowledge, refining the agent through a ReST-like method for continuous self-improvement and self-distillation.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Summarize the ReACT - SYNERGIZING REASONING AND ACTING paper for me\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to calculate the area of a parallelogram with the given dimensions and angle, then write the result to a file.\n",
      "Action: Parallelogram Area Calculator\n",
      "Action Input: 7|6|30\u001b[0m\u001b[33;1m\u001b[1;3m20.999999999999996\u001b[0m\u001b[32;1m\u001b[1;3mParsing LLM output produced both a final answer and a parse-able action:: I have calculated the area of the parallelogram.\n",
      "Action: Write File with content\n",
      "Action Input: ./area.txt|The area of the parallelogram is 20.999999999999996\n",
      "Final Answer: The area of the parallelogram with side 7 and 6 with an angle of 30 degrees is approximately 21 and has been written to a file called area.txt.\u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the area of a parallelogram with side 7 and 6 with an angle of 30 degrees and write the results to a file called area.txt',\n",
       " 'output': 'Agent stopped due to iteration limit or time limit.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Calculate the area of a parallelogram with side 7 and 6 with an angle of 30 degrees and write the results to a file called area.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search for weather information in Los Angeles in December.\n",
      "Action: Wikidata\n",
      "Action Input: Los Angeles\u001b[0m\u001b[33;1m\u001b[1;3mResult Q16910:\n",
      "Label: Los Ángeles\n",
      "Description: city in Chile\n",
      "Aliases: Los Angeles, Villa Los Ángeles\n",
      "instance of: city in Chile, big city\n",
      "country: Chile\n",
      "located in the administrative territorial entity: Los Ángeles\n",
      "inception: 1739\n",
      "coordinate location: -37.4707, -72.3517\n",
      "\n",
      "Result Q65:\n",
      "Label: Los Angeles\n",
      "Description: largest city in California, United States of America\n",
      "Aliases: Los Angeles, California, Pink City, The town of Our Lady the Queen of the Angels of the Little Portion, La La Land, Tinsel Town, City of Angels, City of Los Angeles, LA, California, L.A., LA, Double Dubuque, Los Ángeles, Los Angeles, CA, L.A\n",
      "instance of: charter city, city in the United States, city, global city, metropolis, megacity, business cluster, big city\n",
      "country: United States of America\n",
      "located in the administrative territorial entity: Los Angeles County\n",
      "inception: 1781-09-04\n",
      "coordinate location: 34.05223, -118.24368\u001b[0m\u001b[32;1m\u001b[1;3mI need to specify that I want weather information for Los Angeles, California.\n",
      "Action: Wikidata\n",
      "Action Input: Q65\u001b[0m\u001b[33;1m\u001b[1;3mResult Q65:\n",
      "Label: Los Angeles\n",
      "Description: largest city in California, United States of America\n",
      "Aliases: Los Angeles, California, Pink City, The town of Our Lady the Queen of the Angels of the Little Portion, La La Land, Tinsel Town, City of Angels, City of Los Angeles, LA, California, L.A., LA, Double Dubuque, Los Ángeles, Los Angeles, CA, L.A\n",
      "instance of: charter city, city in the United States, city, global city, metropolis, megacity, business cluster, big city\n",
      "country: United States of America\n",
      "located in the administrative territorial entity: Los Angeles County\n",
      "inception: 1781-09-04\n",
      "coordinate location: 34.05223, -118.24368\n",
      "\n",
      "Result Q504331:\n",
      "Label: Q65\n",
      "Description: Dutch band\n",
      "instance of: musical group\n",
      "country of origin: Netherlands\n",
      "country: Netherlands\n",
      "inception: 1965\n",
      "genre: rock music\n",
      "location of formation: The Hague\u001b[0m\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How is the weather in LA in December',\n",
       " 'output': 'Agent stopped due to iteration limit or time limit.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"How is the weather in LA in December\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Ask your own questions that use the tools you have created. Does the LLM use those tools? If not, what levers can you use?\n",
    "\n",
    "What would happen if you give ReACT access to too many tools?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
