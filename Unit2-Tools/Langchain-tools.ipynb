{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Tools Library\n",
    "Langchain has built integration with a lot of different tools. You can see their current list of integrations below.\n",
    "\n",
    "https://python.langchain.com/docs/integrations/tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arxiv',\n",
       " 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun, ArxivAPIWrapper\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=ArxivAPIWrapper())\n",
    "arxiv_tool.name, arxiv_tool.description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2024-03-14\\nTitle: Me LLaMA: Foundation Large Language Models for Medical Applications\\nAuthors: Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Xingyu Zhou, Huan He, Lucila Ohno-Machado, Yonghui Wu, Hua Xu, Jiang Bian\\nSummary: Recent large language models (LLMs) such as ChatGPT and LLaMA have shown\\ngreat promise in many AI applications. However, their performance on medical\\ntasks is suboptimal and can be improved by training on extensive\\ndomain-specific datasets. This study introduces Me LLaMA, a medical LLM family\\nthat includes foundation models - Me LLaMA 13/70B, along with their\\nchat-enhanced versions - Me LLaMA 13/70B-chat, developed through continual\\npre-training and instruction tuning of LLaMA2 using large medical datasets. Our\\ndomain-specific data suite for training and evaluation includes a large-scale,\\ncontinual pre-training dataset with 129B tokens, an instruction tuning dataset\\nwith 214k samples, and a new medical evaluation benchmark (MIBE) across six\\ntasks with 12 datasets. Our extensive evaluation using the MIBE shows that Me\\nLLaMA models achieve overall better performance than existing open-source\\nmedical LLMs in zero-shot, few-shot and supervised learning abilities. Their\\nzero-shot performance is comparable with ChatGPT across 7 out of 8 datasets,\\nwith a slight variance of within 3%, and yet falls short when compared to\\nGPT-4. In addition, we investigated the catastrophic forgetting problem, and\\nour results show that Me LLaMA models outperform other open-source medical LLMs\\nin mitigating this issue. Me LLaMA is one of the largest open-source medical\\nfoundation LLMs that use both biomedical and clinical data. It exhibits\\nsuperior performance across both general and medical tasks compared to other\\nopen-source medical LLMs, rendering it an attractive choice for medical AI\\napplications. We release our models, datasets, and evaluation scripts at:\\nhttps://github.com/BIDS-Xu-Lab/Me-LLaMA.\\n\\nPublished: 2024-03-08\\nTitle: Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs\\nAuthors: Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott\\nSummary: Text generation with Large Language Models (LLMs) is known to be memory bound\\ndue to the combination of their auto-regressive nature, huge parameter counts,\\nand limited memory bandwidths, often resulting in low token rates. Speculative\\ndecoding has been proposed as a solution for LLM inference acceleration.\\nHowever, since draft models are often unavailable in the modern open-source LLM\\nfamilies, e.g., for Llama 2 7B, training a high-quality draft model is required\\nto enable inference acceleration via speculative decoding. In this paper, we\\npropose a simple draft model training framework for direct alignment to\\nchat-capable target models. With the proposed framework, we train Llama 2 Chat\\nDrafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\\\\% of\\nthe original size. Our training framework only consists of pretraining,\\ndistillation dataset generation, and finetuning with knowledge distillation,\\nwith no additional alignment procedure. For the finetuning step, we use\\ninstruction-response pairs generated by target model for distillation in\\nplausible data distribution, and propose a new Total Variation Distance++\\n(TVD++) loss that incorporates variance reduction techniques inspired from the\\npolicy gradient method in reinforcement learning. Our empirical results show\\nthat Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3\\nblock efficiency and 2.4$\\\\times$ speed-up relative to autoregressive decoding\\non various tasks with no further task-specific fine-tuning.\\n\\nPublished: 2024-02-01\\nTitle: Investigating Bias Representations in Llama 2 Chat via Activation Steering\\nAuthors: Dawn Lu, Nina Rimsky\\nSummary: We address the challenge of societal bias in Large Language Models (LLMs),\\nfocusing on the Llama 2 7B Ch'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_tool.run(\"Tell me about Llama 2: Open Foundation and Fine-Tuned Chat Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikidata Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Wikidata',\n",
       " 'A wrapper around Wikidata. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be the exact name of the item you want information about or a Wikidata QID.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.wikidata.tool import WikidataAPIWrapper, WikidataQueryRun\n",
    "\n",
    "wikidata_tool = WikidataQueryRun(api_wrapper=WikidataAPIWrapper())\n",
    "wikidata_tool.name, wikidata_tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Q7251:\n",
      "Label: Alan Turing\n",
      "Description: English computer scientist (1912–1954)\n",
      "Aliases: Alan M. Turing, Alan Mathieson Turing, Turing, Alan Mathison Turing\n",
      "instance of: human\n",
      "country of citizenship: United Kingdom\n",
      "occupation: computer scientist, mathematician, university teacher, cryptographer, logician, statistician, marathon runner, artificial intelligence researcher\n",
      "sex or gender: male\n",
      "date of birth: 1912-06-23\n",
      "date of death: 1954-06-07\n",
      "sport: athletics\n",
      "place of birth: Maida Vale, Warrington Lodge\n",
      "educated at: King's College, Princeton University, Sherborne School, Hazlehurst Community Primary School\n",
      "employer: Victoria University of Manchester, Government Communications Headquarters, University of Cambridge, National Physical Laboratory\n",
      "place of death: Wilmslow\n",
      "field of work: cryptanalysis, computer science, mathematics, logic, cryptography\n",
      "cause of death: cyanide poisoning\n",
      "notable work: On Computable Numbers, with an Application to the Entscheidungsproblem, Computing Machinery and Intelligence, Intelligent Machinery, halting problem, Turing machine, Turing test, Turing completeness, Church-Turing thesis, universal Turing machine, Symmetric Turing machine, non-deterministic Turing machine, Bombe, probabilistic Turing machine, Turing degree\n",
      "religion or worldview: atheism\n",
      "mother: Ethel Sara Stoney\n",
      "father: Julius Mathison Turing\n",
      "doctoral student: Robin Gandy, Beatrice Helen Worsley\n",
      "student: Robin Gandy\n",
      "\n",
      "Result Q28846012:\n",
      "Label: Alan Turing\n",
      "Description: fictional analogon of Alan Turing (1912-1954)\n",
      "Aliases: Alan Mathison Turing\n",
      "instance of: fictional human\n",
      "sex or gender: male\n"
     ]
    }
   ],
   "source": [
    "print(wikidata_tool.run(\"Alan Turing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('youtube_search',\n",
       " 'search for youtube videos associated with a person. the input to this tool should be a comma separated list, the first part contains a person name and the second a number that is the maximum number of video results to return aka num_results. the second part is optional')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "youtube_tool = YouTubeSearchTool()\n",
    "youtube_tool.name, youtube_tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['https://www.youtube.com/watch?v=eYK7CxcaZw4&pp=ygUPc3RlcGhlbiBjb2xiZXJ0', 'https://www.youtube.com/watch?v=crwjX4uIUFc&pp=ygUPc3RlcGhlbiBjb2xiZXJ0', 'https://www.youtube.com/watch?v=kaH7wj8U4Nw&pp=ygUPc3RlcGhlbiBjb2xiZXJ0', 'https://www.youtube.com/watch?v=ddb104fcptM&pp=ygUPc3RlcGhlbiBjb2xiZXJ0', 'https://www.youtube.com/watch?v=iyPHzpIAhGs&pp=ygUPc3RlcGhlbiBjb2xiZXJ0']\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_tool.run(\"stephen colbert,5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your custom tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File writing tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "class FileTools():\n",
    "\n",
    "  @tool(\"Write File with content\")\n",
    "  def write_file(data):\n",
    "    \"\"\"Useful to write a file to a given path with a given content. \n",
    "       The input to this tool should be a pipe (|) separated text \n",
    "       of length two. The first string is the location of the file and the second is the message in it.\n",
    "       For example, `./observations.txt|REACT_COMPONENT_CODE_PLACEHOLDER`.\n",
    "       Replace REACT_COMPONENT_CODE_PLACEHOLDER with the actual \n",
    "       code you want to write to the file.\"\"\"\n",
    "    try:\n",
    "      path, content = data.split(\"|\")\n",
    "      path = path.replace(\"\\n\", \"\").replace(\" \", \"\").replace(\"`\", \"\")\n",
    "      with open(path, \"w\") as f:\n",
    "        f.write(content)\n",
    "      return f\"File written to {path}.\"\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      return \"Error with the input format for the tool.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Write File with content',\n",
       " 'Write File with content(data) - Useful to write a file to a given path with a given content. \\n       The input to this tool should be a pipe (|) separated text \\n       of length two. The first string is the location of the file and the second is the message in it.\\n       For example, `./observations.txt|REACT_COMPONENT_CODE_PLACEHOLDER`.\\n       Replace REACT_COMPONENT_CODE_PLACEHOLDER with the actual \\n       code you want to write to the file.')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_tool = FileTools.write_file\n",
    "file_tool.name, file_tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File written to ./test.txt.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_tool.run('./test.txt|This is a test for the tool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Calculator tool with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from math import radians, sin\n",
    "\n",
    "desc = (\n",
    "    '''use this tool when you need to calculate the area of a parallelogram.\n",
    "     If the parallelogram is a rectangle then you will be given just width and height.\n",
    "     If it not a rectangle, then you will need the angle between the width and the height. Angle should be in degrees.\n",
    "     To use the tool, you must provide at least two of these 3 parameters- ['width', 'height', 'angle'].\n",
    "     The input to the tool must be provided as width|height|angle or width|height\n",
    "    '''\n",
    ")\n",
    "\n",
    "class AreaCalculator(BaseTool):\n",
    "    name = \"Parallelogram Area Calculator\"\n",
    "    description = desc\n",
    "    \n",
    "    def _run(\n",
    "        self,\n",
    "        input\n",
    "    ):\n",
    "        # check for the values we have been given\n",
    "        extract = input.split(\"|\")\n",
    "        width = float(extract[0])\n",
    "        height = float(extract[1])\n",
    "        if len(extract) == 2:\n",
    "            angle = None\n",
    "        else:\n",
    "            angle= float(extract[2])\n",
    "        if not angle:\n",
    "            return width * height\n",
    "        elif angle:\n",
    "            return width * height * float(sin(radians(angle)))\n",
    "        else:\n",
    "            return \"Could not calculate the area of a parallelogram. Need two or more of `width`, `height`, or `angle`.\"\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "area_tool = AreaCalculator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Parallelogram Area Calculator',\n",
       " \"use this tool when you need to calculate the area of a parallelogram.\\n     If the parallelogram is a rectangle then you will be given just width and height.\\n     If it not a rectangle, then you will need the angle between the width and the height. Angle should be in degrees.\\n     To use the tool, you must provide at least two of these 3 parameters- ['width', 'height', 'angle'].\\n     The input to the tool must be provided as width|height|angle or width|height\\n    \")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_tool.name, area_tool.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.99999999999999"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_tool.run('10|12|30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_tool.run('10|12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Make your own custom tool. Ideas include:\n",
    "- A tool to get the latest weather at a location\n",
    "- HuggingFace tool to run text classification\n",
    "\n",
    "Make sure to set a good description for the tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing multiple tool selection with Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up environment and keys for langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") ## Put your OpenAI API key here\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\") ## Put your Tavily Search API key here\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") ## Put your Langsmith API key here\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") ## Put your Langsmith API key here\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true' ## Set this as True\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = 'https://api.smith.langchain.com/' ## Set this as: https://api.smith.langchain.com/\n",
    "os.environ[\"LANGCHAIN_HUB_API_URL\"] = 'https://api.hub.langchain.com' ## Set this as : https://api.hub.langchain.com\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'llm-agents'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReACT Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReACT Paper - https://arxiv.org/pdf/2210.03629.pdf\n",
    "\n",
    "![ReACT Loop](images/ReAct_loop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0125',\n",
    "                 temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [arxiv_tool, wikidata_tool, youtube_tool, file_tool, area_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Wikidata to find information about Beethoven.\n",
      "Action: Wikidata\n",
      "Action Input: Beethoven\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Error response when hitting Wikibase: Error(code='invalid-item-id', message='Not a valid item ID: <wikibase_rest_api_client.types.Unset object at 0x1235ba960>', additional_properties={})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3mResult Q255:\n",
      "Label: Ludwig van Beethoven\n",
      "Description: German composer (1770–1827)\n",
      "Aliases: Beethoven, L. van Beethoven, Ludvig van Beethoven, Ludwig Van Beethoven, L. Bethovenas, Ludwik van Beethoven, Louis van Beethoven, L. v. Beethoven, L. van Betkhoven, Ludwig von Beethoven\n",
      "instance of: human\n",
      "occupation: composer, pianist, conductor, music teacher, organist, virtuoso, improviser, violinist, writer\n",
      "sex or gender: male\n",
      "date of birth: 1770-12-16, 1770-12-17, 1770\n",
      "date of death: 1827-03-26, 1827-03-26, 1827\n",
      "place of birth: Bonn\n",
      "place of death: Vienna\n",
      "field of work: classical music, symphony, string quartet, chamber music, traditional folk song, string trio, opera, music, piano performance\n",
      "cause of death: liver cirrhosis\n",
      "notable work: Für Elise, Symphony No. 9, Piano Sonata No. 14, Missa Solemnis, Piano Sonata No. 8, Symphony No. 5, Symphony No. 6, Piano Sonata No. 21, Piano Sonata No. 23, Violin Sonata No. 9, Symphony No. 3, Fidelio\n",
      "religion or worldview: Catholicism\n",
      "mother: Maria Magdalena van Beethoven\n",
      "father: Johann van Beethoven\n",
      "student: Carl Czerny, Dorothea von Ertmann, Ferdinand Ries, Archduke Rudolf of Austria, Babette Countess Keglevich de Buzin\n",
      "\n",
      "Result Q144207:\n",
      "Label: 1815 Beethoven\n",
      "Description: asteroid\n",
      "Aliases: Beethoven\n",
      "instance of: asteroid\u001b[0m\u001b[32;1m\u001b[1;3mI have found the information about Beethoven, now I need to find out when he was born.\n",
      "Action: Wikidata\n",
      "Action Input: Q255\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Error response when hitting Wikibase: Error(code='invalid-item-id', message='Not a valid item ID: <wikibase_rest_api_client.types.Unset object at 0x1235ba960>', additional_properties={})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3mResult Q255:\n",
      "Label: Ludwig van Beethoven\n",
      "Description: German composer (1770–1827)\n",
      "Aliases: Beethoven, L. van Beethoven, Ludvig van Beethoven, Ludwig Van Beethoven, L. Bethovenas, Ludwik van Beethoven, Louis van Beethoven, L. v. Beethoven, L. van Betkhoven, Ludwig von Beethoven\n",
      "instance of: human\n",
      "occupation: composer, pianist, conductor, music teacher, organist, virtuoso, improviser, violinist, writer\n",
      "sex or gender: male\n",
      "date of birth: 1770-12-16, 1770-12-17, 1770\n",
      "date of death: 1827-03-26, 1827-03-26, 1827\n",
      "place of birth: Bonn\n",
      "place of death: Vienna\n",
      "field of work: classical music, symphony, string quartet, chamber music, traditional folk song, string trio, opera, music, piano performance\n",
      "cause of death: liver cirrhosis\n",
      "notable work: Für Elise, Symphony No. 9, Piano Sonata No. 14, Missa Solemnis, Piano Sonata No. 8, Symphony No. 5, Symphony No. 6, Piano Sonata No. 21, Piano Sonata No. 23, Violin Sonata No. 9, Symphony No. 3, Fidelio\n",
      "religion or worldview: Catholicism\n",
      "mother: Maria Magdalena van Beethoven\n",
      "father: Johann van Beethoven\n",
      "student: Carl Czerny, Dorothea von Ertmann, Ferdinand Ries, Archduke Rudolf of Austria, Babette Countess Keglevich de Buzin\n",
      "\n",
      "Result Q124034935:\n",
      "Label: punishment of woman who prefers mortal lover to gods\n",
      "Description: narrative motif documented in Thompson's Motif-Index of Folk-Literature\n",
      "Aliases: Q255.\n",
      "instance of: narrative motif, folklore motif\u001b[0m\u001b[32;1m\u001b[1;3mI have found the information about Beethoven and his birthdate. \n",
      "Final Answer: Ludwig van Beethoven was born on December 16 or 17, 1770.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'who is Beethoven? When was he born',\n",
       " 'output': 'Ludwig van Beethoven was born on December 16 or 17, 1770.'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"who is Beethoven? When was he born\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the arxiv tool to find and summarize the paper.\n",
      "Action: arxiv\n",
      "Action Input: ReACT - SYNERGIZING REASONING AND ACTING\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2023-12-15\n",
      "Title: ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\n",
      "Authors: Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, Sanjiv Kumar\n",
      "Summary: Answering complex natural language questions often necessitates multi-step\n",
      "reasoning and integrating external information. Several systems have combined\n",
      "knowledge retrieval with a large language model (LLM) to answer such questions.\n",
      "These systems, however, suffer from various failure cases, and we cannot\n",
      "directly train them end-to-end to fix such failures, as interaction with\n",
      "external knowledge is non-differentiable. To address these deficiencies, we\n",
      "define a ReAct-style LLM agent with the ability to reason and act upon external\n",
      "knowledge. We further refine the agent through a ReST-like method that\n",
      "iteratively trains on previous trajectories, employing growing-batch\n",
      "reinforcement learning with AI feedback for continuous self-improvement and\n",
      "self-distillation. Starting from a prompted large model and after just two\n",
      "iterations of the algorithm, we can produce a fine-tuned small model that\n",
      "achieves comparable performance on challenging compositional question-answering\n",
      "benchmarks with two orders of magnitude fewer parameters.\n",
      "\n",
      "Published: 2019-06-27\n",
      "Title: To Act or React: Investigating Proactive Strategies For Online Community Moderation\n",
      "Authors: Hussam Habib, Maaz Bin Musa, Fareed Zaffar, Rishab Nithyanand\n",
      "Summary: Reddit administrators have generally struggled to prevent or contain such\n",
      "discourse for several reasons including: (1) the inability for a handful of\n",
      "human administrators to track and react to millions of posts and comments per\n",
      "day and (2) fear of backlash as a consequence of administrative decisions to\n",
      "ban or quarantine hateful communities. Consequently, as shown in our background\n",
      "research, administrative actions (community bans and quarantines) are often\n",
      "taken in reaction to media pressure following offensive discourse within a\n",
      "community spilling into the real world with serious consequences. In this\n",
      "paper, we investigate the feasibility of proactive moderation on Reddit --\n",
      "i.e., proactively identifying communities at risk of committing offenses that\n",
      "previously resulted in bans for other communities. Proactive moderation\n",
      "strategies show promise for two reasons: (1) they have potential to narrow down\n",
      "the communities that administrators need to monitor for hateful content and (2)\n",
      "they give administrators a scientific rationale to back their administrative\n",
      "decisions and interventions. Our work shows that communities are constantly\n",
      "evolving in their user base and topics of discourse and that evolution into\n",
      "hateful or dangerous (i.e., considered bannable by Reddit administrators)\n",
      "communities can often be predicted months ahead of time. This makes proactive\n",
      "moderation feasible. Further, we leverage explainable machine learning to help\n",
      "identify the strongest predictors of evolution into dangerous communities. This\n",
      "provides administrators with insights into the characteristics of communities\n",
      "at risk becoming dangerous or hateful. Finally, we investigate, at scale, the\n",
      "impact of participation in hateful and dangerous subreddits and the\n",
      "effectiveness of community bans and quarantines on the behavior of members of\n",
      "these communities.\n",
      "\n",
      "Published: 2024-02-06\n",
      "Title: An LLM Compiler for Parallel Function Calling\n",
      "Authors: Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, Amir Gholami\n",
      "Summary: Recent language models have shown remarkable results on various complex\n",
      "reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute\n",
      "external function calls to overcome their inherent limitations, such as\n",
      "knowledge cutoffs, poor arithmetic skills, or lack of access to private data.\n",
      "This development has allowed LLMs to select and coordinate multiple functions\n",
      "based on the context to tack\u001b[0m\u001b[32;1m\u001b[1;3mI should summarize the ReACT - SYNERGIZING REASONING AND ACTING paper.\n",
      "Action: arxiv\n",
      "Action Input: ReACT - SYNERGIZING REASONING AND ACTING\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2023-12-15\n",
      "Title: ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\n",
      "Authors: Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, Sanjiv Kumar\n",
      "Summary: Answering complex natural language questions often necessitates multi-step\n",
      "reasoning and integrating external information. Several systems have combined\n",
      "knowledge retrieval with a large language model (LLM) to answer such questions.\n",
      "These systems, however, suffer from various failure cases, and we cannot\n",
      "directly train them end-to-end to fix such failures, as interaction with\n",
      "external knowledge is non-differentiable. To address these deficiencies, we\n",
      "define a ReAct-style LLM agent with the ability to reason and act upon external\n",
      "knowledge. We further refine the agent through a ReST-like method that\n",
      "iteratively trains on previous trajectories, employing growing-batch\n",
      "reinforcement learning with AI feedback for continuous self-improvement and\n",
      "self-distillation. Starting from a prompted large model and after just two\n",
      "iterations of the algorithm, we can produce a fine-tuned small model that\n",
      "achieves comparable performance on challenging compositional question-answering\n",
      "benchmarks with two orders of magnitude fewer parameters.\n",
      "\n",
      "Published: 2019-06-27\n",
      "Title: To Act or React: Investigating Proactive Strategies For Online Community Moderation\n",
      "Authors: Hussam Habib, Maaz Bin Musa, Fareed Zaffar, Rishab Nithyanand\n",
      "Summary: Reddit administrators have generally struggled to prevent or contain such\n",
      "discourse for several reasons including: (1) the inability for a handful of\n",
      "human administrators to track and react to millions of posts and comments per\n",
      "day and (2) fear of backlash as a consequence of administrative decisions to\n",
      "ban or quarantine hateful communities. Consequently, as shown in our background\n",
      "research, administrative actions (community bans and quarantines) are often\n",
      "taken in reaction to media pressure following offensive discourse within a\n",
      "community spilling into the real world with serious consequences. In this\n",
      "paper, we investigate the feasibility of proactive moderation on Reddit --\n",
      "i.e., proactively identifying communities at risk of committing offenses that\n",
      "previously resulted in bans for other communities. Proactive moderation\n",
      "strategies show promise for two reasons: (1) they have potential to narrow down\n",
      "the communities that administrators need to monitor for hateful content and (2)\n",
      "they give administrators a scientific rationale to back their administrative\n",
      "decisions and interventions. Our work shows that communities are constantly\n",
      "evolving in their user base and topics of discourse and that evolution into\n",
      "hateful or dangerous (i.e., considered bannable by Reddit administrators)\n",
      "communities can often be predicted months ahead of time. This makes proactive\n",
      "moderation feasible. Further, we leverage explainable machine learning to help\n",
      "identify the strongest predictors of evolution into dangerous communities. This\n",
      "provides administrators with insights into the characteristics of communities\n",
      "at risk becoming dangerous or hateful. Finally, we investigate, at scale, the\n",
      "impact of participation in hateful and dangerous subreddits and the\n",
      "effectiveness of community bans and quarantines on the behavior of members of\n",
      "these communities.\n",
      "\n",
      "Published: 2024-02-06\n",
      "Title: An LLM Compiler for Parallel Function Calling\n",
      "Authors: Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, Amir Gholami\n",
      "Summary: Recent language models have shown remarkable results on various complex\n",
      "reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute\n",
      "external function calls to overcome their inherent limitations, such as\n",
      "knowledge cutoffs, poor arithmetic skills, or lack of access to private data.\n",
      "This development has allowed LLMs to select and coordinate multiple functions\n",
      "based on the context to tack\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The ReACT - SYNERGIZING REASONING AND ACTING paper discusses a ReAct-style LLM agent with the ability to reason and act upon external knowledge, refining the agent through a ReST-like method for continuous self-improvement and self-distillation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Summarize the ReACT - SYNERGIZING REASONING AND ACTING paper for me',\n",
       " 'output': 'The ReACT - SYNERGIZING REASONING AND ACTING paper discusses a ReAct-style LLM agent with the ability to reason and act upon external knowledge, refining the agent through a ReST-like method for continuous self-improvement and self-distillation.'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Summarize the ReACT - SYNERGIZING REASONING AND ACTING paper for me\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to calculate the area of a parallelogram with the given dimensions and angle, then write the result to a file.\n",
      "Action: Parallelogram Area Calculator\n",
      "Action Input: 7|6|30\u001b[0m\u001b[33;1m\u001b[1;3m20.999999999999996\u001b[0m\u001b[32;1m\u001b[1;3mI have calculated the area of the parallelogram.\n",
      "Action: Write File with content\n",
      "Action Input: ./area.txt|The area of the parallelogram is 20.999999999999996\u001b[0m\u001b[36;1m\u001b[1;3mFile written to ./area.txt.\u001b[0m\u001b[32;1m\u001b[1;3mI have completed the task.\n",
      "Final Answer: The area of the parallelogram with side 7 and 6 and an angle of 30 degrees is approximately 21 and has been written to a file named area.txt.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the area of a parallelogram with side 7 and 6 with an angle of 30 degrees and write the results to a file called area.txt',\n",
       " 'output': 'The area of the parallelogram with side 7 and 6 and an angle of 30 degrees is approximately 21 and has been written to a file named area.txt.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Calculate the area of a parallelogram with side 7 and 6 with an angle of 30 degrees and write the results to a file called area.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search for weather information in Los Angeles in December.\n",
      "Action: Wikidata\n",
      "Action Input: Los Angeles\u001b[0m\u001b[33;1m\u001b[1;3mResult Q16910:\n",
      "Label: Los Ángeles\n",
      "Description: city in Chile\n",
      "Aliases: Los Angeles, Villa Los Ángeles\n",
      "instance of: city in Chile, big city\n",
      "country: Chile\n",
      "located in the administrative territorial entity: Los Ángeles\n",
      "inception: 1739\n",
      "coordinate location: -37.4707, -72.3517\n",
      "\n",
      "Result Q65:\n",
      "Label: Los Angeles\n",
      "Description: largest city in California, United States of America\n",
      "Aliases: Los Angeles, California, Pink City, The town of Our Lady the Queen of the Angels of the Little Portion, La La Land, Tinsel Town, City of Angels, City of Los Angeles, LA, California, L.A., LA, Double Dubuque, Los Ángeles, Los Angeles, CA, L.A\n",
      "instance of: charter city, city in the United States, city, global city, metropolis, megacity, business cluster, big city\n",
      "country: United States of America\n",
      "located in the administrative territorial entity: Los Angeles County\n",
      "inception: 1781-09-04\n",
      "coordinate location: 34.05223, -118.24368\u001b[0m\u001b[32;1m\u001b[1;3mI need to specify that I want weather information for Los Angeles in December.\n",
      "Action: Wikidata\n",
      "Action Input: Q65\u001b[0m\u001b[33;1m\u001b[1;3mResult Q65:\n",
      "Label: Los Angeles\n",
      "Description: largest city in California, United States of America\n",
      "Aliases: Los Angeles, California, Pink City, The town of Our Lady the Queen of the Angels of the Little Portion, La La Land, Tinsel Town, City of Angels, City of Los Angeles, LA, California, L.A., LA, Double Dubuque, Los Ángeles, Los Angeles, CA, L.A\n",
      "instance of: charter city, city in the United States, city, global city, metropolis, megacity, business cluster, big city\n",
      "country: United States of America\n",
      "located in the administrative territorial entity: Los Angeles County\n",
      "inception: 1781-09-04\n",
      "coordinate location: 34.05223, -118.24368\n",
      "\n",
      "Result Q504331:\n",
      "Label: Q65\n",
      "Description: Dutch band\n",
      "instance of: musical group\n",
      "country of origin: Netherlands\n",
      "country: Netherlands\n",
      "inception: 1965\n",
      "genre: rock music\n",
      "location of formation: The Hague\u001b[0m\u001b[32;1m\u001b[1;3mI need to search for weather information specifically for Los Angeles in December.\n",
      "Action: arxiv\n",
      "Action Input: Los Angeles weather December\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2021-04-22\n",
      "Title: Scalable Predictive Time-Series Analysis of COVID-19: Cases and Fatalities\n",
      "Authors: Shradha Shinde, Jay Joshi, Sowmya Mareedu, Yeon Pyo Kim, Jongwook Woo\n",
      "Summary: COVID 19 is an acute disease that started spreading throughout the world,\n",
      "beginning in December 2019. It has spread worldwide and has affected more than\n",
      "7 million people, and 200 thousand people have died due to this infection as of\n",
      "Oct 2020. In this paper, we have forecasted the number of deaths and the\n",
      "confirmed cases in Los Angeles and New York of the United States using the\n",
      "traditional and Big Data platforms based on the Times Series: ARIMA and ETS. We\n",
      "also implemented a more sophisticated time-series forecast model using Facebook\n",
      "Prophet API. Furthermore, we developed the classification models: Logistic\n",
      "Regression and Random Forest regression to show that the Weather does not\n",
      "affect the number of the confirmed cases. The models are built and run in\n",
      "legacy systems (Azure ML Studio) and Big Data systems (Oracle Cloud and\n",
      "Databricks). Besides, we present the accuracy of the models.\n",
      "\n",
      "Published: 2011-08-03\n",
      "Title: Point process modeling of wildfire hazard in Los Angeles County, California\n",
      "Authors: Haiyong Xu, Frederic Paik Schoenberg\n",
      "Summary: The Burning Index (BI) produced daily by the United States government's\n",
      "National Fire Danger Rating System is commonly used in forecasting the hazard\n",
      "of wildfire activity in the United States. However, recent evaluations have\n",
      "shown the BI to be less effective at predicting wildfires in Los Angeles\n",
      "County, compared to simple point process models incorporating similar\n",
      "meteorological information. Here, we explore the forecasting power of a suite\n",
      "of more complex point process models that use seasonal wildfire trends, daily\n",
      "and lagged weather variables, and historical spatial burn patterns as\n",
      "covariates, and that interpolate the records from different weather stations.\n",
      "Results are compared with models using only the BI. The performance of each\n",
      "model is compared by Akaike Information Criterion (AIC), as well as by the\n",
      "power in predicting wildfires in the historical data set and residual analysis.\n",
      "We find that multiplicative models that directly use weather variables offer\n",
      "substantial improvement in fit compared to models using only the BI, and, in\n",
      "particular, models where a distinct spatial bandwidth parameter is estimated\n",
      "for each weather station appear to offer substantially improved fit.\n",
      "\n",
      "Published: 2019-12-18\n",
      "Title: A systematic comparison of structural, structural connectivity, and functional connectivity based thalamus parcellation techniques\n",
      "Authors: Charles Iglehart, Martin Monti, Joshua Cain, Thomas Tourdias, Manojkumar Saranathan\n",
      "Summary: The thalamus consists of several histologically and functionally distinct\n",
      "nuclei increasingly implicated in brain pathology and important for treatment,\n",
      "motivating the need for development of fast and accurate thalamic segmentation.\n",
      "The contrast between thalamic nuclei as well as between the thalamus and\n",
      "surrounding tissues is poor in T1 and T2 weighted magnetic resonance imaging\n",
      "(MRI), inhibiting efforts to date to segment the thalamus using standard\n",
      "clinical MRI. Automatic segmentation techniques have been developed to leverage\n",
      "thalamic features better captured by advanced MRI methods, including\n",
      "magnetization prepared rapid acquisition gradient echo (MP-RAGE) , diffusion\n",
      "tensor imaging (DTI), and resting state functional MRI (fMRI). Despite\n",
      "operating on fundamentally different image features, these methods claim a high\n",
      "degree of agreement with the Morel stereotactic atlas of the thalamus. However,\n",
      "no comparison has been undertaken to compare the results of these disparate\n",
      "segmentation methods. We have implemented state-of-the-art structural,\n",
      "diffusion, and functional imaging-based thalamus segmentation techniques and\n",
      "used them on a single set of subjects. We present the first systematic\n",
      "qualitative and quantitative comparison of these methods. We found that\n",
      "fun\u001b[0m\u001b[32;1m\u001b[1;3mI need to be more specific in my search for weather information in Los Angeles in December.\n",
      "Action: arxiv\n",
      "Action Input: Los Angeles weather December forecast\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2021-04-22\n",
      "Title: Scalable Predictive Time-Series Analysis of COVID-19: Cases and Fatalities\n",
      "Authors: Shradha Shinde, Jay Joshi, Sowmya Mareedu, Yeon Pyo Kim, Jongwook Woo\n",
      "Summary: COVID 19 is an acute disease that started spreading throughout the world,\n",
      "beginning in December 2019. It has spread worldwide and has affected more than\n",
      "7 million people, and 200 thousand people have died due to this infection as of\n",
      "Oct 2020. In this paper, we have forecasted the number of deaths and the\n",
      "confirmed cases in Los Angeles and New York of the United States using the\n",
      "traditional and Big Data platforms based on the Times Series: ARIMA and ETS. We\n",
      "also implemented a more sophisticated time-series forecast model using Facebook\n",
      "Prophet API. Furthermore, we developed the classification models: Logistic\n",
      "Regression and Random Forest regression to show that the Weather does not\n",
      "affect the number of the confirmed cases. The models are built and run in\n",
      "legacy systems (Azure ML Studio) and Big Data systems (Oracle Cloud and\n",
      "Databricks). Besides, we present the accuracy of the models.\n",
      "\n",
      "Published: 2011-08-03\n",
      "Title: Point process modeling of wildfire hazard in Los Angeles County, California\n",
      "Authors: Haiyong Xu, Frederic Paik Schoenberg\n",
      "Summary: The Burning Index (BI) produced daily by the United States government's\n",
      "National Fire Danger Rating System is commonly used in forecasting the hazard\n",
      "of wildfire activity in the United States. However, recent evaluations have\n",
      "shown the BI to be less effective at predicting wildfires in Los Angeles\n",
      "County, compared to simple point process models incorporating similar\n",
      "meteorological information. Here, we explore the forecasting power of a suite\n",
      "of more complex point process models that use seasonal wildfire trends, daily\n",
      "and lagged weather variables, and historical spatial burn patterns as\n",
      "covariates, and that interpolate the records from different weather stations.\n",
      "Results are compared with models using only the BI. The performance of each\n",
      "model is compared by Akaike Information Criterion (AIC), as well as by the\n",
      "power in predicting wildfires in the historical data set and residual analysis.\n",
      "We find that multiplicative models that directly use weather variables offer\n",
      "substantial improvement in fit compared to models using only the BI, and, in\n",
      "particular, models where a distinct spatial bandwidth parameter is estimated\n",
      "for each weather station appear to offer substantially improved fit.\n",
      "\n",
      "Published: 2021-05-19\n",
      "Title: Predicting Flight Delay with Spatio-Temporal Trajectory Convolutional Network and Airport Situational Awareness Map\n",
      "Authors: Wei Shao, Arian Prabowo, Sichen Zhao, Piotr Koniusz, Flora D. Salim\n",
      "Summary: To model and forecast flight delays accurately, it is crucial to harness\n",
      "various vehicle trajectory and contextual sensor data on airport tarmac areas.\n",
      "These heterogeneous sensor data, if modelled correctly, can be used to generate\n",
      "a situational awareness map. Existing techniques apply traditional supervised\n",
      "learning methods onto historical data, contextual features and route\n",
      "information among different airports to predict flight delay are inaccurate and\n",
      "only predict arrival delay but not departure delay, which is essential to\n",
      "airlines. In this paper, we propose a vision-based solution to achieve a high\n",
      "forecasting accuracy, applicable to the airport. Our solution leverages a\n",
      "snapshot of the airport situational awareness map, which contains various\n",
      "trajectories of aircraft and contextual features such as weather and airline\n",
      "schedules. We propose an end-to-end deep learning architecture, TrajCNN, which\n",
      "captures both the spatial and temporal information from the situational\n",
      "awareness map. Additionally, we reveal that the situational awareness map of\n",
      "the airport has a vital impact on estimating flight departure delay. Our\n",
      "proposed framework obtained a good result (around 18 minutes error) for\n",
      "predicting flight departure delay at Los Angeles International Airport.\u001b[0m\u001b[32;1m\u001b[1;3mI need to be more specific in my search for weather information in Los Angeles in December.\n",
      "Action: arxiv\n",
      "Action Input: Los Angeles weather December forecast 2021\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2021-04-22\n",
      "Title: Scalable Predictive Time-Series Analysis of COVID-19: Cases and Fatalities\n",
      "Authors: Shradha Shinde, Jay Joshi, Sowmya Mareedu, Yeon Pyo Kim, Jongwook Woo\n",
      "Summary: COVID 19 is an acute disease that started spreading throughout the world,\n",
      "beginning in December 2019. It has spread worldwide and has affected more than\n",
      "7 million people, and 200 thousand people have died due to this infection as of\n",
      "Oct 2020. In this paper, we have forecasted the number of deaths and the\n",
      "confirmed cases in Los Angeles and New York of the United States using the\n",
      "traditional and Big Data platforms based on the Times Series: ARIMA and ETS. We\n",
      "also implemented a more sophisticated time-series forecast model using Facebook\n",
      "Prophet API. Furthermore, we developed the classification models: Logistic\n",
      "Regression and Random Forest regression to show that the Weather does not\n",
      "affect the number of the confirmed cases. The models are built and run in\n",
      "legacy systems (Azure ML Studio) and Big Data systems (Oracle Cloud and\n",
      "Databricks). Besides, we present the accuracy of the models.\n",
      "\n",
      "Published: 2011-08-03\n",
      "Title: Point process modeling of wildfire hazard in Los Angeles County, California\n",
      "Authors: Haiyong Xu, Frederic Paik Schoenberg\n",
      "Summary: The Burning Index (BI) produced daily by the United States government's\n",
      "National Fire Danger Rating System is commonly used in forecasting the hazard\n",
      "of wildfire activity in the United States. However, recent evaluations have\n",
      "shown the BI to be less effective at predicting wildfires in Los Angeles\n",
      "County, compared to simple point process models incorporating similar\n",
      "meteorological information. Here, we explore the forecasting power of a suite\n",
      "of more complex point process models that use seasonal wildfire trends, daily\n",
      "and lagged weather variables, and historical spatial burn patterns as\n",
      "covariates, and that interpolate the records from different weather stations.\n",
      "Results are compared with models using only the BI. The performance of each\n",
      "model is compared by Akaike Information Criterion (AIC), as well as by the\n",
      "power in predicting wildfires in the historical data set and residual analysis.\n",
      "We find that multiplicative models that directly use weather variables offer\n",
      "substantial improvement in fit compared to models using only the BI, and, in\n",
      "particular, models where a distinct spatial bandwidth parameter is estimated\n",
      "for each weather station appear to offer substantially improved fit.\n",
      "\n",
      "Published: 2021-05-19\n",
      "Title: Predicting Flight Delay with Spatio-Temporal Trajectory Convolutional Network and Airport Situational Awareness Map\n",
      "Authors: Wei Shao, Arian Prabowo, Sichen Zhao, Piotr Koniusz, Flora D. Salim\n",
      "Summary: To model and forecast flight delays accurately, it is crucial to harness\n",
      "various vehicle trajectory and contextual sensor data on airport tarmac areas.\n",
      "These heterogeneous sensor data, if modelled correctly, can be used to generate\n",
      "a situational awareness map. Existing techniques apply traditional supervised\n",
      "learning methods onto historical data, contextual features and route\n",
      "information among different airports to predict flight delay are inaccurate and\n",
      "only predict arrival delay but not departure delay, which is essential to\n",
      "airlines. In this paper, we propose a vision-based solution to achieve a high\n",
      "forecasting accuracy, applicable to the airport. Our solution leverages a\n",
      "snapshot of the airport situational awareness map, which contains various\n",
      "trajectories of aircraft and contextual features such as weather and airline\n",
      "schedules. We propose an end-to-end deep learning architecture, TrajCNN, which\n",
      "captures both the spatial and temporal information from the situational\n",
      "awareness map. Additionally, we reveal that the situational awareness map of\n",
      "the airport has a vital impact on estimating flight departure delay. Our\n",
      "proposed framework obtained a good result (around 18 minutes error) for\n",
      "predicting flight departure delay at Los Angeles International Airport.\u001b[0m\u001b[32;1m\u001b[1;3mI have not found specific weather information for Los Angeles in December.\n",
      "Action: arxiv\n",
      "Action Input: Los Angeles weather December 2021 forecast\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2021-04-22\n",
      "Title: Scalable Predictive Time-Series Analysis of COVID-19: Cases and Fatalities\n",
      "Authors: Shradha Shinde, Jay Joshi, Sowmya Mareedu, Yeon Pyo Kim, Jongwook Woo\n",
      "Summary: COVID 19 is an acute disease that started spreading throughout the world,\n",
      "beginning in December 2019. It has spread worldwide and has affected more than\n",
      "7 million people, and 200 thousand people have died due to this infection as of\n",
      "Oct 2020. In this paper, we have forecasted the number of deaths and the\n",
      "confirmed cases in Los Angeles and New York of the United States using the\n",
      "traditional and Big Data platforms based on the Times Series: ARIMA and ETS. We\n",
      "also implemented a more sophisticated time-series forecast model using Facebook\n",
      "Prophet API. Furthermore, we developed the classification models: Logistic\n",
      "Regression and Random Forest regression to show that the Weather does not\n",
      "affect the number of the confirmed cases. The models are built and run in\n",
      "legacy systems (Azure ML Studio) and Big Data systems (Oracle Cloud and\n",
      "Databricks). Besides, we present the accuracy of the models.\n",
      "\n",
      "Published: 2011-08-03\n",
      "Title: Point process modeling of wildfire hazard in Los Angeles County, California\n",
      "Authors: Haiyong Xu, Frederic Paik Schoenberg\n",
      "Summary: The Burning Index (BI) produced daily by the United States government's\n",
      "National Fire Danger Rating System is commonly used in forecasting the hazard\n",
      "of wildfire activity in the United States. However, recent evaluations have\n",
      "shown the BI to be less effective at predicting wildfires in Los Angeles\n",
      "County, compared to simple point process models incorporating similar\n",
      "meteorological information. Here, we explore the forecasting power of a suite\n",
      "of more complex point process models that use seasonal wildfire trends, daily\n",
      "and lagged weather variables, and historical spatial burn patterns as\n",
      "covariates, and that interpolate the records from different weather stations.\n",
      "Results are compared with models using only the BI. The performance of each\n",
      "model is compared by Akaike Information Criterion (AIC), as well as by the\n",
      "power in predicting wildfires in the historical data set and residual analysis.\n",
      "We find that multiplicative models that directly use weather variables offer\n",
      "substantial improvement in fit compared to models using only the BI, and, in\n",
      "particular, models where a distinct spatial bandwidth parameter is estimated\n",
      "for each weather station appear to offer substantially improved fit.\n",
      "\n",
      "Published: 2021-05-19\n",
      "Title: Predicting Flight Delay with Spatio-Temporal Trajectory Convolutional Network and Airport Situational Awareness Map\n",
      "Authors: Wei Shao, Arian Prabowo, Sichen Zhao, Piotr Koniusz, Flora D. Salim\n",
      "Summary: To model and forecast flight delays accurately, it is crucial to harness\n",
      "various vehicle trajectory and contextual sensor data on airport tarmac areas.\n",
      "These heterogeneous sensor data, if modelled correctly, can be used to generate\n",
      "a situational awareness map. Existing techniques apply traditional supervised\n",
      "learning methods onto historical data, contextual features and route\n",
      "information among different airports to predict flight delay are inaccurate and\n",
      "only predict arrival delay but not departure delay, which is essential to\n",
      "airlines. In this paper, we propose a vision-based solution to achieve a high\n",
      "forecasting accuracy, applicable to the airport. Our solution leverages a\n",
      "snapshot of the airport situational awareness map, which contains various\n",
      "trajectories of aircraft and contextual features such as weather and airline\n",
      "schedules. We propose an end-to-end deep learning architecture, TrajCNN, which\n",
      "captures both the spatial and temporal information from the situational\n",
      "awareness map. Additionally, we reveal that the situational awareness map of\n",
      "the airport has a vital impact on estimating flight departure delay. Our\n",
      "proposed framework obtained a good result (around 18 minutes error) for\n",
      "predicting flight departure delay at Los Angeles International Airport.\u001b[0m\u001b[32;1m\u001b[1;3mI have exhausted my search options and have not found specific weather information for Los Angeles in December.\n",
      "Final Answer: Specific weather information for Los Angeles in December is not available through the sources searched.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How is the weather in LA in December',\n",
       " 'output': 'Specific weather information for Los Angeles in December is not available through the sources searched.'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"How is the weather in LA in December\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Ask your own questions that use the tools you have created. Does the LLM use those tools? If not, what levers can you use?\n",
    "\n",
    "What would happen if you give ReACT access to too many tools?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
