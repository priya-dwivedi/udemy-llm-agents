{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langgraph Basics\n",
    "\n",
    "![lang-graph](images/langgraph.jpg)\n",
    "\n",
    "What is LangGraph?\n",
    "\n",
    "LangGraph is a library built on top of LangChain, designed to add cyclic computational capabilities to your LLM applications. While LangChain allows you to define chains of computation (Directed Acyclic Graphs or DAGs), LangGraph introduces the ability to add cycles, enabling more complex, agent-like behaviors where you can call an LLM in a loop, asking it what action to take next.\n",
    "\n",
    "* Stateful Graph: LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses.\n",
    "\n",
    "* Nodes: Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step. You define nodes to perform specific tasks, such as processing input, making decisions, or interacting with external APIs.\n",
    "\n",
    "* Edges: Edges connect the nodes in your graph, defining the flow of computation. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph\n",
    "\n",
    "\n",
    "Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141\n",
    "\n",
    "\n",
    "In LangGraph, both MessageGraph and StateGraph are used to represent and manage the state of a graph during its execution. However, they differ in how they structure and update this state.\n",
    "\n",
    "### MessageGraph:\n",
    "\n",
    "* State Structure: The state is a simple list of messages.\n",
    "* Update Mechanism: Each node's output (typically a message) is appended to the state list after execution.\n",
    "* Ideal Use Case: Best suited for applications involving conversational AI or chatbots, where the state primarily consists of a history of messages exchanged between the user and the AI.\n",
    "\n",
    "### StateGraph:\n",
    "\n",
    "* State Structure: The state is a more general dictionary (or object), where different keys can store different types of information relevant to the graph's execution.\n",
    "* Update Mechanism: Each node can update any part of the state dictionary based on its output and logic.\n",
    "* Ideal Use Case: Offers greater flexibility, making it suitable for a wider range of applications where the state needs to store more complex information beyond just messages, such as user preferences, environment variables, or intermediate results.\n",
    "\n",
    "#### Key Difference:\n",
    "\n",
    "The main difference lies in the structure and flexibility of the state. MessageGraph is a specialized version of StateGraph, optimized for scenarios where the state is primarily a conversation history. StateGraph provides more flexibility for storing and updating arbitrary state information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Langgraph example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+  \n",
      "| __start__ |  \n",
      "+-----------+  \n",
      "       *       \n",
      "       *       \n",
      "       *       \n",
      "+------------+ \n",
      "| greet_node | \n",
      "+------------+ \n",
      "       *       \n",
      "       *       \n",
      "       *       \n",
      "+-----------+  \n",
      "| user_node |  \n",
      "+-----------+  \n",
      "       *       \n",
      "       *       \n",
      "       *       \n",
      "  +---------+  \n",
      "  | __end__ |  \n",
      "  +---------+  \n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import Graph\n",
    "\n",
    "def greet(input):\n",
    "    return \"Hello \"\n",
    "\n",
    "def user(input):\n",
    "    return input + \"Sam!\"\n",
    "    \n",
    "#Create a graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "#Add python functions as nodes\n",
    "workflow.add_node(\"greet_node\", greet)\n",
    "workflow.add_node(\"user_node\", user)\n",
    "\n",
    "#Add an edge\n",
    "workflow.add_edge(\"greet_node\", \"user_node\")\n",
    "\n",
    "workflow.set_entry_point(\"greet_node\")\n",
    "workflow.set_finish_point(\"user_node\")\n",
    "\n",
    "app = workflow.compile()\n",
    "app.get_graph().print_ascii()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'greet_node':\n",
      "---\n",
      "Hello \n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'user_node':\n",
      "---\n",
      "Hello Sam!\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = 'Hi'\n",
    "for output in app.stream(input):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langggraph example with LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") ## Put your OpenAI API key here\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") ## Put your Langsmith API key here\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") ## Put your Langsmith API key here\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true' ## Set this as True\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = 'https://api.smith.langchain.com/' ## Set this as: https://api.smith.langchain.com/\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'llm-langgraph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  +-----------+    \n",
      "  | __start__ |    \n",
      "  +-----------+    \n",
      "         *         \n",
      "         *         \n",
      "         *         \n",
      "  +-------------+  \n",
      "  | Create_Joke |  \n",
      "  +-------------+  \n",
      "         *         \n",
      "         *         \n",
      "         *         \n",
      "+----------------+ \n",
      "| Criticize_Joke | \n",
      "+----------------+ \n",
      "         *         \n",
      "         *         \n",
      "         *         \n",
      "    +---------+    \n",
      "    | __end__ |    \n",
      "    +---------+    \n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import Graph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the model as ChatOpenAI\n",
    "model = ChatOpenAI(temperature=0) \n",
    "\n",
    "def joker(input):\n",
    "    prompt = f\"Tell me a joke about {input}\"\n",
    "    response = model.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "def critic(input):\n",
    "    prompt = f\"Rate and Criticize this joke: {input}\"\n",
    "    response = model.invoke(prompt)\n",
    "    return response.content\n",
    "    \n",
    "#Create a graph\n",
    "\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"Create_Joke\", joker)\n",
    "workflow.add_node(\"Criticize_Joke\", critic)\n",
    "\n",
    "workflow.add_edge(\"Create_Joke\", \"Criticize_Joke\")\n",
    "\n",
    "workflow.set_entry_point(\"Create_Joke\")\n",
    "workflow.set_finish_point(\"Criticize_Joke\")\n",
    "\n",
    "app = workflow.compile()\n",
    "app.get_graph().print_ascii()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'Create_Joke':\n",
      "---\n",
      "Why did the Canadian cross the road?\n",
      "\n",
      "To get to the Tim Hortons on the other side, eh!\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'Criticize_Joke':\n",
      "---\n",
      "I would rate this joke a 6 out of 10. It plays on the stereotype of Canadians loving Tim Hortons, which can be funny to some people. However, it is a bit predictable and not very original. Adding the \"eh\" at the end is a nice touch to emphasize the Canadian stereotype, but overall the joke could use a bit more creativity.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = 'Canadians'\n",
    "for output in app.stream(input):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
