{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7oT5LEa-c54"
      },
      "source": [
        "## Skeleton of Thought method of generation\n",
        "\n",
        "LLM decoding is normally sequential. In this method of decoding, the LLM first generates a skeleton of the response. Then it elaborates on each point in the skeleton concurrently.\n",
        "\n",
        "This method of decoding resembles how humans approach a problem - First generate the outline of a solution and then do parallel processing\n",
        "\n",
        "You can read more in my blog here: https://generativeai.pub/skeleton-of-thought-processing-0980d9b75f52\n",
        "\n",
        "![skeleton-of-thought-process.png](images/sot.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw3l4YdP_4ga"
      },
      "source": [
        "## Skeleton of thought step by step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zFACeK_v_els"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") ## Put your OpenAI API key here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nCRsDYhJACjY"
      },
      "outputs": [],
      "source": [
        "class Gpt4Turbo:\n",
        "    def __init__(self):\n",
        "        self.MODEL = 'gpt-3.5-turbo'\n",
        "        self.TOKEN_LIMIT=4000\n",
        "        self.client = OpenAI()\n",
        "\n",
        "    def gptCall_json(self, temperature, messages: list):\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(model=self.MODEL,\n",
        "                                                    messages=messages,\n",
        "                                                    temperature=temperature,\n",
        "                                                    max_tokens=self.TOKEN_LIMIT,\n",
        "                                                    stream=False,\n",
        "                                                    response_format={\"type\": \"json_object\"}) ## Enforce output format\n",
        "\n",
        "            output = response.choices[0].message.content\n",
        "            return output\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdj0dnzqAGxu"
      },
      "source": [
        "## Prompt to generate the skeleon outline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L5LCRN5ZAGMG"
      },
      "outputs": [],
      "source": [
        "question = \"How can I improve my time management skills?\"\n",
        "outline_prompt = f'''\n",
        "You're an organizer responsible for only giving the skeleton (not the full content) for answering the question.\n",
        "Provide the skeleton as a JSON to answer the question. Instead of writing a full sentence, each skeleton point should\n",
        "be very short with only 2~5 words. Generally, the skeleton should have 3~10 points. The skeleton is an outline that would be expanded later.\n",
        "Don't elaborate on the point in the skeleton.\n",
        "Example:\n",
        "\\n\\nQuestion:\\nWhat are the typical types of Chinese dishes?: \\n Response: {{\"answer\" : [\"Dumplings\" , \"Noodles\" , \"Dim Sum\" , \"Hot Pot\" , \"Wonton\", \"Ma Po Tofu\", \"Char Siu\", \" Fried Rice\"]}}.\n",
        "\\n\\nQuestion:\\nWhat are some practical tips for individuals to reduce their carbon emissions?\\n Response: {{ \"answer\" :[\"Energy Conservation\", \"Efficient transportation\", \"Home Energy Efficiency\", \"Reduce Water Consumption\", \"Sustainable Diet\", \"Sustainable Travel\"]}}\n",
        "\n",
        " \\n\\nNow, please provide the skeleton for the following question.\\n{question}\\n Response: {{\"answer\": [...]}}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ru4OQ7DAO97",
        "outputId": "d201eb1d-51dc-42a8-d1df-62c60ba54657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Set goals', 'Prioritize tasks', 'Create a schedule', 'Limit distractions', 'Delegate when possible', 'Take breaks', 'Use time management tools']\n"
          ]
        }
      ],
      "source": [
        "TEMPERATURE=0.5\n",
        "message=[]\n",
        "\n",
        "message.append({\"role\": \"system\", \"content\": \"You are a helpful assistant. You respond in JSON format.\"})\n",
        "message.append({\"role\": \"user\", \"content\": outline_prompt})\n",
        "\n",
        "\n",
        "final_output = []\n",
        "gpt4_turbo = Gpt4Turbo()\n",
        "result = gpt4_turbo.gptCall_json(TEMPERATURE,message)\n",
        "result = json.loads(result)['answer']\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV-TNK0uAP2u"
      },
      "source": [
        "Nice! So we got the model to five us a skeleton of the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW2os7hAAXew"
      },
      "source": [
        "### Prompt to elaborate on a point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "orO6bC4OAPM_"
      },
      "outputs": [],
      "source": [
        "point = result[0]\n",
        "point_prompt = f'''\n",
        "You help elaborate on the point user wants. Your input is a question and one possible answer from the question, also called <point>. You will elaborate on the <point> and give a 2-3 sentence response\n",
        "on how the <point> helps answer the question. Start your response by mentioning the <point> and then colon like point: and then your response\n",
        "Your response will be in JSON format. Example: {{\"answer\": {point}: your response\"}}\n",
        "\\n\\nNow, please elaborate on the following point. Question: {question}\\n <Point> : {point} \\n Response: {{\"answer\": [...]}}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VavyRWFAeP3",
        "outputId": "ce542c10-ee44-4c60-f9e7-3c11db8baf8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"answer\": \"Set goals: Setting goals is crucial for improving time management skills as it provides a clear direction and purpose for your tasks. By setting specific, measurable, achievable, relevant, and time-bound (SMART) goals, you can prioritize your activities, stay focused, and track your progress effectively.\"}\n"
          ]
        }
      ],
      "source": [
        "TEMPERATURE=0.3\n",
        "message=[]\n",
        "\n",
        "message.append({\"role\": \"system\", \"content\": \"You are a helpful assistant. You respond in JSON format.\"})\n",
        "message.append({\"role\": \"user\", \"content\": point_prompt})\n",
        "\n",
        "\n",
        "gpt4_turbo = Gpt4Turbo()\n",
        "result = gpt4_turbo.gptCall_json(TEMPERATURE,message)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgBTtu76Ag0K"
      },
      "source": [
        "## Putting both together including concurrent calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "er3HWOv8Aehk"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Upccbz6rAopt"
      },
      "outputs": [],
      "source": [
        "class Gpt4Turbo:\n",
        "    def __init__(self):\n",
        "        self.MODEL = 'gpt-3.5-turbo-1106'\n",
        "        self.TOKEN_LIMIT=4000\n",
        "        self.client = OpenAI()\n",
        "        self.temperature =0.3\n",
        "        self.streaming = False\n",
        "\n",
        "    def gptCall_json(self, temperature, messages: list):\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(model=self.MODEL,\n",
        "                                                    messages=messages,\n",
        "                                                    temperature=temperature,\n",
        "                                                    max_tokens=self.TOKEN_LIMIT,\n",
        "                                                    stream=False,\n",
        "                                                    response_format={\"type\": \"json_object\"}) ## Enforce output format\n",
        "\n",
        "\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            return \"\"\n",
        "\n",
        "    def generate_skeleton(self):\n",
        "        question = self.question\n",
        "        outline_prompt = f'''\n",
        "        You're an organizer responsible for only giving the skeleton (not the full content) for answering the question.\n",
        "        Provide the skeleton as a JSON to answer the question. Instead of writing a full sentence, each skeleton point should\n",
        "        be very short with only 2~5 words. Generally, the skeleton should have 3~10 points. The skeleton is an outline that would be expanded later.\n",
        "        Don't elaborate on the point in the skeleton.\n",
        "        Example:\n",
        "        \\n\\nQuestion:\\nWhat are the typical types of Chinese dishes?: \\n Response: {{\"answer\" : [\"Dumplings\" , \"Noodles\" , \"Dim Sum\" , \"Hot Pot\" , \"Wonton\", \"Ma Po Tofu\", \"Char Siu\", \" Fried Rice\"]}}.\n",
        "        \\n\\nQuestion:\\nWhat are some practical tips for individuals to reduce their carbon emissions?\\n Response: {{ \"answer\" :[\"Energy Conservation\", \"Efficient transportation\", \"Home Energy Efficiency\", \"Reduce Water Consumption\", \"Sustainable Diet\", \"Sustainable Travel\"]}}\n",
        "\n",
        "        \\n\\nNow, please provide the skeleton for the following question.\\n{question}\\n Response: {{\"answer\": [...]}}\n",
        "        '''\n",
        "\n",
        "        ## Make the message\n",
        "        message=[]\n",
        "        message.append({\"role\": \"system\", \"content\": \"You are a helpful assistant. You respond in JSON format.\"})\n",
        "        message.append({\"role\": \"user\", \"content\": outline_prompt})\n",
        "\n",
        "        result = self.gptCall_json(self.temperature, message)\n",
        "        result = json.loads(result)\n",
        "        self.result = result['answer']\n",
        "\n",
        "\n",
        "    def elaborate_point(self, point):\n",
        "\n",
        "        question = self.question\n",
        "\n",
        "        point_prompt = f'''\n",
        "        You help elaborate on the point user wants. Your input is a question and one possible answer from the question, also called <point>. You will elaborate on the <point> and give a 2-3 sentence response\n",
        "        on how the <point> helps answer the question. Start your response by mentioning the <point> and then colon like point: and then your response\n",
        "        Your response will be in JSON format. Example: {{\"answer\": {point}: your response\"}}\n",
        "        \\n\\nNow, please elaborate on the following point. Question: {question}\\n <Point> : {point} \\n Response: {{\"answer\": [...]}}\n",
        "        '''\n",
        "\n",
        "        ## Make the message\n",
        "        message=[]\n",
        "        message.append({\"role\": \"system\", \"content\": \"You are a helpful assistant. You respond in JSON format.\"})\n",
        "        message.append({\"role\": \"user\", \"content\": point_prompt})\n",
        "\n",
        "        result = self.gptCall_json(self.temperature, message)\n",
        "        point_elaborate = json.loads(result)\n",
        "        return point_elaborate['answer']\n",
        "\n",
        "\n",
        "    def concurrent_results(self, question):\n",
        "        self.question = question\n",
        "        self.generate_skeleton()\n",
        "        num_points = len(self.result)\n",
        "        # Create a thread pool executor with 5 threads\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_points) as executor:\n",
        "            # Submit the API calls to the executor\n",
        "            outputs = [executor.submit(self.elaborate_point, point) for point in self.result]\n",
        "            # Wait for the API calls to complete and get the results\n",
        "            results = [future.result() for future in concurrent.futures.as_completed(outputs)]\n",
        "\n",
        "        # Use list comprehension to add enumeration and \"\\n\" each record\n",
        "        string_list = [f\"{i+1}. {record}\\n\" for i, record in enumerate(results)]\n",
        "\n",
        "        # Join the string_list elements into a single string\n",
        "        final_output = ''.join(string_list)\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukvdqitZArcf",
        "outputId": "ef451e22-1738-428e-b3c5-feda498a6ce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Using productivity tools can help you manage your time more effectively by providing features such as task lists, reminders, and time tracking. These tools can help you prioritize tasks, set deadlines, and track your progress, ultimately leading to better time management and increased productivity.\n",
            "2. Set goals: Setting goals helps you prioritize your tasks and allocate time effectively. By defining clear objectives, you can focus on activities that align with your goals, leading to better time management and productivity.\n",
            "3. Take breaks: Taking breaks can actually improve productivity and time management. By giving yourself short breaks, you can recharge and maintain focus, ultimately leading to better time management and efficiency in completing tasks.\n",
            "4. Prioritize tasks: Prioritizing tasks helps you focus on the most important and urgent activities, allowing you to allocate your time and resources effectively. By identifying and tackling high-priority tasks first, you can ensure that you are making the best use of your time and achieving your goals efficiently.\n",
            "5. Time blocking: Time blocking is a time management technique where you schedule specific blocks of time for different tasks or activities. This helps you focus on one task at a time, reduces multitasking, and increases productivity by creating a structured approach to managing your time effectively.\n",
            "6. Limit distractions: By minimizing distractions such as social media, email notifications, and unnecessary meetings, you can create a focused environment that allows you to allocate your time more effectively. This helps you prioritize tasks and stay on track with your schedule, ultimately leading to better time management.\n",
            "7. Review and adjust: This point is crucial for time management as it emphasizes the need to regularly assess how you are spending your time and make necessary changes. By reviewing your schedule and tasks, you can identify inefficiencies and areas for improvement, allowing you to optimize your time and increase productivity.\n",
            "8. Delegate tasks: Delegating tasks allows you to focus on high-priority activities and ensures that work is distributed efficiently among team members. By assigning responsibilities to others, you can free up time to concentrate on strategic planning and important decision-making, ultimately leading to better time management.\n",
            "\n",
            "CPU times: user 65.4 ms, sys: 13.2 ms, total: 78.6 ms\n",
            "Wall time: 2.78 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "gpt4_turbo = Gpt4Turbo()\n",
        "question = \"How do I best manage my time?\"\n",
        "result_sot = gpt4_turbo.concurrent_results(question)\n",
        "print(result_sot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt8Zw_zEBOjC",
        "outputId": "cf244668-a8be-4bed-8369-5408341e17e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens: 420\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "num_tokens = len(encoding.encode(result_sot))\n",
        "print(f\"Number of tokens: {num_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNdeIf_QA7iU"
      },
      "source": [
        "It took 4.7s to generate an output that was 360 tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_4g9VEjlFX8"
      },
      "source": [
        "### General ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afRAzt56A6o1",
        "outputId": "db290bee-0eaf-4f99-b18c-e59848e775a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Set clear goals and prioritize tasks based on their importance and deadlines', 'Create a daily schedule or to-do list to stay organized and focused', 'Use time management tools such as calendars, planners, or apps to track your activities', 'Break large tasks into smaller, manageable chunks to avoid feeling overwhelmed', 'Minimize distractions by setting specific times for checking emails and social media', 'Delegate tasks when possible to free up time for more important responsibilities', 'Take regular breaks to avoid burnout and maintain productivity', 'Evaluate your time management regularly and make adjustments as needed to improve efficiency']\n",
            "CPU times: user 13.4 ms, sys: 4.86 ms, total: 18.2 ms\n",
            "Wall time: 2.91 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "gpt4_turbo = Gpt4Turbo()\n",
        "\n",
        "message=[]\n",
        "message.append({\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond in JSON format\"})\n",
        "message.append({\"role\": \"user\", \"content\": f'Answer the user question below as a LONG answer of atleast 8 sentences. Give the answer in bullets.  Question: {question}. Answer: {{\"answer\" : ...}}'})\n",
        "\n",
        "single_result = gpt4_turbo.gptCall_json(temperature=0.3, messages=message)\n",
        "single_result = json.loads(single_result)\n",
        "print(single_result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbyj2UvXlWPs",
        "outputId": "fc8a1992-e2d0-49f2-9324-3b92d83e8594"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected string or buffer",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m      2\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mencoding_for_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/llm_agents/lib/python3.12/site-packages/tiktoken/core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[1;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m \u001b[43m_special_token_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[38;5;241m.\u001b[39mgroup())\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "num_tokens = len(encoding.encode(single_result['answer']))\n",
        "print(f\"Number of tokens: {num_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuMFABpAl92L"
      },
      "source": [
        "It took 3.6s to generate an output that was 356 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGpTafZYwgSR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
